{"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"N7bnDuXGLCKi"},"source":["# Text Classification\n","\n","In this assignment, you will work on the [OffensEval](https://sites.google.com/site/offensevalsharedtask/) shared task. This challenge has been part of the 2019 and 2020 editions of SemEval and focuses on the identification of offensive language in social media platforms. In particular, you are solving subtasks A and B of the 2019 edition:\n","\n","* **SubTask A: Offensive language identification.** The goal of this subtask is to discriminate between offensive and non-offensive posts. Offensive posts include insults, threats, and other type of non-acceptable language. This subtask can be addressed as a Binary Text Classication problem.\n","\n","\n","* **SubTask B: Automatic categorization of offense types.** The goal is to predict if the offensive post is targeted or not. A post is considered targeted if it contains insults or threats to an individual or group. An untargeted offensive post contains non-acceptable language that is not targeted at anyone in particular. In this assignment, you will work on a version of this subtask where the goal is to classify posts as targeted, untargeted and non-offensive. This version of the subtask can be addressed as a Multiclass Text Classication problem.\n","\n","You will work with [scikit-learn](https://scikit-learn.org/stable/), a **Python** Machine Learning library that provides a wide range of tools, including some for text data. Specifically, you will use the following objects and functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"owb0L7xTLCKm"},"outputs":[],"source":["import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import f1_score, accuracy_score"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Vh4PTpmvLCKn"},"source":["The data for the assignment consists of 13240 tweets for training and 860 tweets for test with annotations for both **SubTask A** (*True* or *False*) and **SubTask B** (*TIN*, *UNT* and *NOT*). The dataset also includes the Sentiment Analysis of the tweets that you will use later in the assignment. The dataset can be loaded into two `DataFrames` as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"XPdIe-SnLCKo"},"outputs":[],"source":["train = pd.read_csv(\"data/train.tsv\", sep=\"\\t\")\n","test = pd.read_csv(\"data/test.tsv\", sep=\"\\t\")\n","train[[\"tweet\", \"sentiment\", \"subtask_a\", \"subtask_b\"]]"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"-g-4X6LwLCKo"},"source":["## Text Representation - [ 6 Marks]\n","\n","In order to apply Text Classification for both subtasks, we first need to convert the text of the tweets into numerical feature vectors. For this assignment, you are using a bag-of-words based on tf-idf. This representation can be obtained with **scikit-learn** using [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer). `TfidfVectorizer` provides a number of pre-processing steps, such as tokenization and stop-words, and other options to represent the text.\n","\n","You must complete the code for the `create_tfidfvectorizer` function. The function must create and return a `TfidfVectorizer` with all parameters at their default value. Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer) to learn how."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFGJ0EkYLCKp"},"outputs":[],"source":["def create_tfidfvectorizer():   # 3 Marks\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"xTUb7DL8LCKp"},"outputs":[],"source":["vectorizer = create_tfidfvectorizer()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Z-bZhAsJLCKp"},"source":["Now that you have created the `TfidfVectorizer`, the next step is to apply it to the dataset and get the representation of the tweets. For this, `TfidfVectorizer` first needs to learn the vocabulary and idf values from the train set. Then, it can be used to transform both the train and test sets. When applied to the text data, `TfidfVectorizer` will pre-process it according to the parameters used.\n","\n","You must complete the code for the `run_vectorizer` function. The function takes the vectorizer created previously, and the tweets of the train and test sets. The function should train the vectorizer on the train text to learn the vocabulary and the idf values, and apply it to transform both the train and test tweets. The expected output is the result of these transformations where each tweet should be represented with a vector of 19083 dimensions:\n","> Shape of train input data: (13240, 19083)  \n","Shape of test input data: (860, 19083)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVGzD7nCLCKq"},"outputs":[],"source":["def run_vectorizer(vectorizer, train, test):   # 3 Marks\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"23C5-HMhLCKq"},"outputs":[],"source":["train_x, test_x = run_vectorizer(vectorizer, train[\"tweet\"], test[\"tweet\"])\n","print(f\"Shape of train input data: {train_x.get_shape()}\")\n","print(f\"Shape of test input data: {test_x.get_shape()}\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"yIOkH2MXLCKr"},"source":["## Logistic Regression - [8 Marks]\n","\n","Having obtained the feature vectors from the text, you can proceed with training a classifier to make predictions about the offensive language of a tweet. You will begin by creating a [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) with **scikit-learn**. To keep the exercise simple, you are going to use the default options which include the *one-vs-all* strategy for the Multiclass case and the [Limited-memory BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) algorithm for optimization. The [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) allows implementing a Logistic Regression classifier that works with Stochastic Gradient Descent, but you won't use it for this assignment.\n","\n","You must complete the code for the `create_model` function. The functions should create and return a `LogisticRegression` with the default options, just increase the maximum number of training iterations to 1000 to ensure that the model converges. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BC4om8scLCKr"},"outputs":[],"source":["def create_model():   # 3 Marks\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"yJ3x_pfmLCKr"},"outputs":[],"source":["model = create_model()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"iGCjOLvTLCKs"},"source":["Just based on the target labels you use for training, the `LogisticRegression` you have created is able to automatically recognize the type of classification problem you are working on, Binary or Multiclass. In the following exercise, you will implement the code to train the model and make predictions on the test set. The same solution will be used for both **SubTask A** and **SubTask B**.\n","\n","You must complete the code for the `run_model` function. The function takes input as the model, the train feature vectors, the train target labels and the test feature vectors. The function should train the model using the train features and labels, and return the predictions for the given test. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfRYuhppLCKs"},"outputs":[],"source":["def run_model(model, train_x, train_y, test_x):   # 5 Marks\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"Lxxcd-ogLCKs"},"outputs":[],"source":["prediction = run_model(model, train_x, train[\"subtask_a\"], test_x)\n","test['prediction_a'] = prediction\n","test[['id', 'tweet', 'subtask_a', 'prediction_a']]"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"vliW1HTrLCKs"},"outputs":[],"source":["prediction = run_model(model, train_x, train[\"subtask_b\"], test_x)\n","test['prediction_b'] = prediction\n","test[['id', 'tweet', 'subtask_b', 'prediction_b']]"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"9KmGPo6ZLCKs"},"source":["You can now evaluate the performance of the `LogisticRegression` on the test set by computing different metrics with the true labels and the predictions obtained in the previous step. **SubTask A** can be evaluated with `accuracy` and `binary f1`, while for **SubTask B** `micro f1` and `macro f1` can be applied. If all went well, you should see results like the following:\n","> \\*\\*\\* SubTask A \\*\\*\\*  \n","accuracy: 0.80  \n","binary f1: 0.49  \n",">\n","> \\*\\*\\* SubTask B \\*\\*\\*    \n","micro f1: 0.78  \n","macro f1: 0.46"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"Qp4mT-mVLCKt"},"outputs":[],"source":["print(\"*** SubTask A ***\")\n","print(f\"accuracy: {accuracy_score(test['subtask_a'], test['prediction_a']):0.2f}\")\n","print(f\"binary f1: {f1_score(test['subtask_a'], test['prediction_a'], average='binary'):0.2f}\")\n","print(\"\")\n","print(\"*** SubTask B ***\")\n","print(f\"micro f1: {f1_score(test['subtask_b'], test['prediction_b'], average='micro'):0.2f}\")\n","print(f\"macro f1: {f1_score(test['subtask_b'], test['prediction_b'], average='macro'):0.2f}\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"CHGuatt4LCKt"},"source":["## Balancing the Dataset - [3 Marks]\n","\n","The differences observed between the metrics used in the above evaluation indicate that the **OfensEval** dataset is not balanced. In **SubTask A**, getting an `accuracy` much higher than the `binary f1` can mean that the number of `False` cases is larger than the number of `True` cases. Similarly, obtaining very different `micro f1` and `macro f1` scores in **SubTask B** is a hint that some of the classes are more frequent than others. This can be verified with the following code lines:\n",">```python \n","train.groupby(by=\"subtask_a\")[[\"tweet\"]].count().reset_index()\n","\n","|    | subtask_a   |   tweet |\n","|---:|:------------|--------:|\n","|  0 | False       |    8840 |\n","|  1 | True        |    4400 |\n","\n",">```python \n","train.groupby(by=\"subtask_b\")[[\"tweet\"]].count().reset_index()\n","\n","|    | subtask_b   |   tweet |\n","|---:|:------------|--------:|\n","|  0 | NOT         |    8840 |\n","|  1 | TIN         |    3876 |\n","|  2 | UNT         |     524 |\n","\n","One solution that can mitigate this problem is to assign weights to the classes in a way that reduces the influence of the most frequent ones. **Scikit-learn** allows easily applying such approach by setting the appropriate option when creating the model. The goal of the next exercise is to create a new version of the `LogisticRegression` that handles the unbalanced dataset better.\n","\n","You must complete the code for the `create_balanced_model` function. The function should create and return a `LogisticRegression` equal to the one created by `create_model` with the only difference being that this version automatically adjusts class weights. Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to learn which parameter to set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gdEDM7CLCKt"},"outputs":[],"source":["def create_balanced_model():   # 3 Marks\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"Mcv2-t1ELCKt"},"outputs":[],"source":["balanced_model = create_balanced_model()\n","prediction = run_model(balanced_model, train_x, train[\"subtask_a\"], test_x)\n","test['prediction_balanced_a'] = prediction\n","test[['id', 'tweet', 'subtask_a', 'prediction_balanced_a']]"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"60QKAW5YLCKt"},"outputs":[],"source":["prediction = run_model(balanced_model, train_x, train[\"subtask_b\"], test_x)\n","test['prediction_balanced_b'] = prediction\n","test[['id', 'tweet', 'subtask_b', 'prediction_balanced_b']]"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"cUiesVXLLCKt"},"source":["The new model should reduce the differences between `accuracy` and `binary f1` and `micro f1` and `macro f1` respectively. You will observe some decrease in `accuracy` and `micro f1` scores, but at the same time `binary f1` and `macro f1` will improve significantly:\n","\n","> \\*\\*\\* SubTask A \\*\\*\\*  \n","accuracy: 0.78  \n","binary f1: 0.61  \n",">\n","> \\*\\*\\* SubTask B \\*\\*\\*  \n","micro f1: 0.75  \n","macro f1: 0.59"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"Kr4t-7UpLCKt"},"outputs":[],"source":["print(\"*** SubTask A ***\")\n","print(f\"accuracy: {accuracy_score(test['subtask_a'], test['prediction_balanced_a']):0.2f}\")\n","print(f\"binary f1: {f1_score(test['subtask_a'], test['prediction_balanced_a'], average='binary'):0.2f}\")\n","print(\"\")\n","print(\"*** SubTask B ***\")\n","print(f\"micro f1: {f1_score(test['subtask_b'], test['prediction_balanced_b'], average='micro'):0.2f}\")\n","print(f\"macro f1: {f1_score(test['subtask_b'], test['prediction_balanced_b'], average='macro'):0.2f}\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"sruWQSBYLCKu"},"source":["## Additional Features - [3 Marks]\n","\n","When working with linear classifiers for Text Classification, if additional information related to the input texts is available, it is often a good idea to incorporate this information in the form of additional features to the text representation. In this assignment, the result of a Sentiment Analysis on the tweets is provided in the *sentiment* column of the `DataFrames`. The goal of this last exercise is to incorporate this information into the input vectors of the `LogisticRegression`.\n","\n","There are different ways to achieve this using **scikit-learn**, but a very handy approach, especially in combination with **pandas** `DataFrame`, is to create a [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html). A `ColumnTransformer` can apply different encoding approaches to different columns of the input data separately and concatenate them to generate a single feature vector.\n","\n","You must complete the code for the `create_column_transformer` function. The function must create and return a `ColumnTransformer` with two transformers: \n","\n","*  A `TfidfVectorizer` that should be applied to the text of the tweets.\n","*  A [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) that should encode the annotations of the Sentiment Analysis.\n","\n","You must use the default parameters for both transformers. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NFy3FbQ-LCKu"},"outputs":[],"source":["def create_column_transformer():   # 3 Marks\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"IjuHS56PLCKu"},"source":["The `ColumnTransformer` can now be run using the `run_vectorizer` function you implemented above. Notice that, in this case, the whole train and test `DataFrames` are passed to `run_vectorizer` along with the `ColumnTransformer`. However, the code of the function should be able to train and run it. The output of the new feature extraction strategy should be a vector of 19086 dimensions per tweet, 3 more dimensions that the previous approach:\n","\n","> Shape of train input data: (13240, 19086)  \n","Shape of test input data: (860, 19086)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"y6vsZKHsLCKu"},"outputs":[],"source":["column_transformer = create_column_transformer()\n","train_x_sentiment, test_x_sentiment = run_vectorizer(column_transformer, train, test)\n","print(f\"Shape of train input data: {train_x_sentiment.get_shape()}\")\n","print(f\"Shape of test input data: {test_x_sentiment.get_shape()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"SgBaUgRvLCKu"},"outputs":[],"source":["prediction = run_model(balanced_model, train_x_sentiment, train[\"subtask_a\"], test_x_sentiment)\n","test['prediction_sentiment_a'] = prediction\n","test[['id', 'tweet', 'subtask_a', 'prediction_sentiment_a']]"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"ybS40mPNLCKu"},"outputs":[],"source":["prediction = run_model(balanced_model, train_x_sentiment, train[\"subtask_b\"], test_x_sentiment)\n","test['prediction_sentiment_b'] = prediction\n","test[['id', 'tweet', 'subtask_b', 'prediction_sentiment_b']]"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"jiA7iOFYLCKu"},"source":["The addition of the Sentiment Analysis to the input feature vector should help in both **SubTask A** and **SubTask B**. All the metrics should get some improvement, especially `binary f1` and `macro f1`:\n","\n","> \\*\\*\\* SubTask A \\*\\*\\*  \n","accuracy: 0.79  \n","binary f1: 0.66    \n",">\n","> \\*\\*\\* SubTask B \\*\\*\\*  \n","micro f1: 0.77  \n","macro f1: 0.62"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"TI86qArGLCKu"},"outputs":[],"source":["print(\"*** SubTask A ***\")\n","print(f\"accuracy: {accuracy_score(test['subtask_a'], test['prediction_sentiment_a'] ):0.2f}\")\n","print(f\"binary f1: {f1_score(test['subtask_a'], test['prediction_sentiment_a'], average='binary'):0.2f}\")\n","print(\"\")\n","print(\"*** SubTask B ***\")\n","print(f\"micro f1: {f1_score(test['subtask_b'], test['prediction_sentiment_b'], average='micro'):0.2f}\")\n","print(f\"macro f1: {f1_score(test['subtask_b'], test['prediction_sentiment_b'], average='macro'):0.2f}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
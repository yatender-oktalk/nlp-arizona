{"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"yrAk1SLm3kvL"},"source":["# Sequence Labeling\n","\n","In this assignment, you will work on the [MeasEval](https://competitions.codalab.org/competitions/25770) shared task that was part of SemEval-2021. The goal of **MeasEval** is  the extraction of counts, measurements, and related context from scientific documents. The task is a complex problem that involves solving a number of steps that range from identifying quantities and units of measurement to identify relationships between them. For this assignment, you will focus only on the *Quantity* recognition step: \n","\n","*  Given a paragraph from a scientific text, identify all spans containing quantities like *12 kg*. This problem can be approached as a Sequence Labeling task.\n","\n","You will develop a Recurrent Neural Network with [Keras](https://keras.io/), a high-level Deep Learning API written in **Python** that provides a user-friendly interface for the [TensorFlow](https://www.tensorflow.org/) library, one of the most popular low-level Deep Learning frameworks. You will use the following objects and functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"tags":["imports"],"id":"cTgxw_Hv3kvU"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from tensorflow.keras.utils import set_random_seed\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, TimeDistributed\n","from tensorflow.keras.initializers import Constant\n","from sklearn.metrics import classification_report"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Fev5y1hq3kvX"},"source":["When working with Neural Networks, there are a large number of random operations such as initializing the weights of the network, shuffling the data for training, or choosing samples. This causes that different training runs of the same model can lead to different results. To ensure reproducibility, i.e. obtaining the same results in the different runs, the random number generator must be initialized with a fixed value known as seed. In `Keras`, this can be done as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"uFi4i-vW3kvX"},"outputs":[],"source":["seed = 42\n","set_random_seed(seed)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"gqE8uXrH3kvY"},"source":["When developing a model, if the results you get are not as expected, try re-initializing the seed by running the cell above before compiling and training the model.\n","\n","> **Note!** With models as complex as Neural Networks, reproducibility is susceptible to factors such as software versions or the hardware on which the models are run. Even with seed initialization, there may be slight differences in the results.\n","\n","Working with Neural Networks also involves defining a number of hyperparameters that set the configuration of the model. Finding the appropriate hyperparameter values requires training the model with different combinations and testing them on the development set. This hyperparameter tuning is a costly process that needs multiple rounds of experimentation. However, for this assignments, you will use the following values:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"RY_0DO7u3kvZ"},"outputs":[],"source":["maxlen = 130  # Maximum length of the input sequence accepted by the model\n","epochs = 6  # Number of epochs to train the model\n","batch_size = 64  # Number of examples used per gradient update\n","embedding_dim = 300  # Dimension of the embeddings\n","rnn_units = 256  # Number of units per RNN layer"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"10wQp_6O3kva"},"source":["Training a Deep Learning model with a large train set can be a time-consuming process, as the model needs to iterate over the entire set multiple times, often requiring significant computational resources. During the implementation of the model, it is often a good practice to use only a subset of the training data. This allows a faster debugging of the code. Set the `shrink_dataset` variable as `True` when a faster training is required and set it as `False` to train the model on the whole train set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmLsM2qY3kva"},"outputs":[],"source":["shrink_dataset = False"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"0Y3EOqrw3kvb"},"source":["Although the value of this variable does not affect the tests that will evaluate your code, the output examples distributed throughout this notebook are based on a `shrink_dataset` variable set as `False`.\n","\n","The train set for the assignment consists of 248 articles with 1366 sentences in total. The test set contains 136 articles with 848 sentences. A development set with 65 documents and 459 sentences is also provided. The dataset is annotated at the token level following a BIO schema with 3 labels: *B-Quantity*, *I-Quantity* and *O*.  The dataset can be loaded into three `DataFrames` as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"yKAuq9L53kvc"},"outputs":[],"source":["def load_data(data_path, shrink_dataset, seed):\n","    data = pd.read_csv(data_path, sep=\"\\t\", encoding=\"utf8\").dropna()\n","    if shrink_dataset:\n","        sample = data[[\"docId\",  \"sentId\"]].drop_duplicates().sample(frac=0.2, random_state=seed)\n","        data = pd.merge(data, sample, on=[\"docId\", \"sentId\"])\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"US-mGMnF3kvd"},"outputs":[],"source":["train_data = load_data(\"data/train.tsv\", shrink_dataset, seed)\n","dev_data = load_data(\"data/trial.tsv\", shrink_dataset, seed)\n","test_data = load_data(\"data/eval.tsv\", shrink_dataset, seed)\n","train_data[(train_data.docId == \"S0378383912000130-3601\") & (train_data.sentId == 3)]"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"veKfPWzA3kvd"},"source":["The `DataFrames` created include the lemmatization of words in the `lemma` columns. You will use the lemmas as the input of the model."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"OM5BHaJz3kvd"},"source":["## Data Pre-processing\n","\n","In this assignment, you will have to implement some steps to pre-process and obtain a representation of the data. You will implement a model with an `Embedding` lookup table as the input layer, so the tokens of the input sentences should be represented as indexes. The target labels should also be represented in similar way. Besides, as one would expect, the sentences in the **MeasEval** dataset have different lengths. However, the input for a Deep Learning model is a batch of examples (in this case, sentences) in the form of a single tensor which requires that all examples in the batch must have the same length. Therefore, the sentences should be padded or truncated to a specific length.\n","\n","> **Note!** For this particular task, the `maxlen` value provided to you guarantees that padding is sufficient to make all sentences the same length without the need for truncation. \n","\n","This first of these pre-processing steps will be to obtain both a vocabulary and the set of labels from the train set. The vocabulary should be the list of unique lemmas and must include the special tokens `[PAD]`, that will be used for padding the sequences, and `[UNK]`, that will be used to represent out-of-vocabulary words. Along with the vocabulary and the label set, you will also have to build a dictionary mapping each lemma to its position in the vocabulary and a dictionary mapping each label to its position in the label set. These dictionaries will be used later to obtain the representation of the input and output of the model. The text is already tokenized and lemmatized which will help in this task.\n","\n","You must complete the code for the `get_vocabulary` function that takes as input the `DataFrame` containing the train set. The function should create a list with the all the unique lemmas and include the special tokens `[PAD]` and `[UNK]` in the first two positions. Similarly, the function should create a list with the unique labels with the special token `[PAD]` in the first position. The **pandas** library provides some [functions](https://pandas.pydata.org/docs/reference/index.html) that may help you. Along with those lists, `get_vocabulary` should return the dictionaries mapping the lemmas and the labels to their corresponding positions. In total, the vocabulary and the label set should have 5508 and 4 items respectively:\n","\n","> Vocabulary size: 5508  \n","Vocabulary first 5 lemmas: ['[PAD]', '[UNK]', 'datum', 'be', 'draw']  \n","Vocabulary dictionary: {'[PAD]': 0, '[UNK]': 1, 'datum': 2, 'be': 3, 'draw': 4}  \n",">\n",">Labels size: 4  \n","Labels: ['[PAD]', 'O', 'B-Quantity', 'I-Quantity']  \n","Labels dictionary: {'[PAD]': 0, 'O': 1, 'B-Quantity': 2, 'I-Quantity': 3}  \n"]},{"cell_type":"markdown","source":["## **test_get_vocabulary = 3 Marks** "],"metadata":{"id":"b9lLR7tGEWSZ"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":["get_vocabulary"],"id":"TiIXRYVD3kvd"},"outputs":[],"source":["def get_vocabulary(train_data):\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"StSo9npQ3kvf"},"outputs":[],"source":["vocab, word2idx, labels, label2idx = get_vocabulary(train_data)\n","vocab_size = len(vocab)\n","label_size = len(labels)\n","print(f\"Vocabulary size: {vocab_size}\")\n","print(f\"Vocabulary first 5 words: {vocab[:5]}\")\n","print(f\"Vocabulary dictionary: { {w: word2idx[w] for w in vocab[:5]}}\")\n","print(\"\")\n","print(f\"Labels size: {label_size}\")\n","print(f\"Labels: {labels}\")\n","print(f\"Labels dictionary: {label2idx}\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"KcOJxtef3kvg"},"source":["Since the *Quantity* recognition task is a Sequence Labeling problem, the input for the model must be the sequence of lemmas in the sentence and the output the sequence of labels. Therefore, the train, development and test `DataFrames` must be reformated by aggregating the data corresponding to each sentence. The `integrate_sentences` will do this for you. The output of `integrate_sentences` is a `DataFrame` with a row for each sentence and the columns `lemmas` and `labels` that contain the list of lemmas and the list of labels of the sentences respectively.  "]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"tags":["sentence_integrate"],"id":"g_io0LoL3kvg"},"outputs":[],"source":["def integrate_sentences(data):\n","    agg_func = lambda s: [s['lemma'].values.tolist(), s['label'].values.tolist()]\n","    data = data.groupby([\"docId\", \"sentId\"], sort=False).apply(agg_func).reset_index().rename(columns={0: 'lemmas_labels'})\n","    data['lemmas'] = data.apply(lambda x: x['lemmas_labels'][0], axis=1)\n","    data['labels'] = data.apply(lambda x: x['lemmas_labels'][1], axis=1)\n","    data = data.drop(columns=\"lemmas_labels\")\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"LqWFA7Ln3kvg"},"outputs":[],"source":["train_examples = integrate_sentences(train_data)\n","dev_examples = integrate_sentences(dev_data)\n","test_examples = integrate_sentences(test_data)\n","pd.set_option('display.max_colwidth', None)\n","train_examples[(train_examples.docId == \"S0378383912000130-3601\") & (train_examples.sentId == 3)]"]},{"cell_type":"markdown","metadata":{"id":"EslW3eIM3kvh"},"source":["The dataset is now ready for you to get the numerical representation of both input and output. You must perform two steps to process the sequence of lemmas and the sequence of labels:\n","1. For each sentence, translate each lemma or label to its corresponding index using the `word2idx` and `label2idx` dictionaries. In case the lemma is not found in `word2idx`, use the index of the `[UNK]` token instead.\n","2. Pad both the sequences of lemmas and the sequences of labels to the same length as defined by the `maxlen` variable. For this, you should use the [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) function with its default padding strategy. This function uses `0` as the default padding value which corresponds to the index of the `[PAD]` token in the vocabulary.   \n","\n","You must complete the code for the `format_examples` function. This function takes as input a `DataFrame` in the format returned by `integrate_sentences`, the `word2idx` and `label2idx` dictionaries, and the `maxlen` variable. The function must run the steps described above and return a **numpy** array with the processed lemma sequences and a **numpy** array with the processed label sequences that will be used as input and output of the model respectively. Applying `format_examples` to the train, development and test sets should result on 6 arrays with the following shapes:\n","\n",">Shape of train input data :  (1366, 130)  \n","Shape of train output data :  (1366, 130)  \n","Shape of development input data :  (459, 130)  \n","Shape of development output data :  (459, 130)  \n","Shape of test input data :  (848, 130)  \n","Shape of test output data :  (848, 130)  "]},{"cell_type":"markdown","source":["## **test_format_examples = 3 Marks**"],"metadata":{"id":"ifVT0qJlM1_P"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":["format_examples"],"id":"-NOYMzSN3kvh"},"outputs":[],"source":["def format_examples(data, word2idx, label2idx, maxlen):\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"SXhHrMi-3kvi"},"outputs":[],"source":["x_train, y_train = format_examples(train_examples, word2idx, label2idx, maxlen)\n","x_dev, y_dev = format_examples(dev_examples, word2idx, label2idx, maxlen)\n","x_test, y_test = format_examples(test_examples, word2idx, label2idx, maxlen)\n","print(\"Shape of train input data: \", x_train.shape)\n","print(\"Shape of train output data: \", y_train.shape)\n","print(\"Shape of development input data: \", x_dev.shape)\n","print(\"Shape of development output data: \", y_dev.shape)\n","print(\"Shape of train input data: \", x_test.shape)\n","print(\"Shape of test output data: \", y_test.shape)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"_yPowN8i3kvi"},"source":["## Recurrent Neural Network\n","\n","There are three ways to create a neural network with **Kerars**: using the [Functional API](https://keras.io/guides/functional_api/), by [Model subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) or creating a [Sequential model](https://keras.io/guides/sequential_model/). In this assignment, you will use the latter option. A `Sequential` model is a straightforward approach to build simple neural networks by stacking the layers. You will construct a RNN with the following 3 layers:\n","1. An [Embedding](https://keras.io/api/layers/core_layers/embedding/) layer with an input dimension equal to the vocabulary size, an embedding dimension defined by `embedding_dim` and where the length of the input sequences is equal to `maxlen`. The layer must also mask out the padding values so that they are not considered when computing the loss.\n","2. A Bidirectional [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/) layer with a number of units determined by `rnn_units`. Since you are working on Sequence Labeling, the `LSTM` must return outputs for the full sequence. To make it Bidirectional, the `LSTM` must be wrapped by a [Bidirectional](https://keras.io/api/layers/recurrent_layers/bidirectional/) layer.\n","3. A [Dense](https://keras.io/api/layers/core_layers/dense/) layer with a number of units equal to the number of labels and a `softmax` activation function.  Since you are working on Sequence Labeling, the `Dense` layer must be wrapped by a [TimeDistributed](https://keras.io/api/layers/recurrent_layers/time_distributed/) layer.\n","\n","You must complete the code for the `create_model` function. This function takes as input the size of the vocabulary, the number of labels and the `maxlen`, `embedding_dim` and `rnn_units` hyperparameters. The function must create a RNN according to the configuration described above. Read carefully all the linked documentation to learn how to create such a model.  Any option not mentioned in the description should be kept with its default value. The summary of the resulting model should look like:\n","\n","\n","> <pre>\n","> Model: \"sequential_1\"\n","> __________________________________________________________________________________________\n","> Layer (type)                           Output Shape                        Param #       \n","> ==========================================================================================\n","> embedding_1 (Embedding)               (None, 130, 300)                    1652400       \n",">                                                                                          \n","> bidirectional_1 (Bidirectional)       (None, 130, 512)                    1140736       \n",">                                                                                          \n","> time_distributed_1 (TimeDistributed)  (None, 130, 4)                      2052          \n",">                                                                                          \n","> ==========================================================================================\n","> Total params: 2,795,188\n","> Trainable params: 2,795,188\n","> Non-trainable params: 0\n","> __________________________________________________________________________________________\n","> </pre>\n","\n","Before returning the model, the `create_model` function should [compile](https://keras.io/api/models/model_training_apis/#compile-method) it using `'sparse_categorical_crossentropy'` as the loss function, `'adam'` as the optimizer and `'sparse_categorical_accuracy'` as a metric to evaluate the model during training."]},{"cell_type":"markdown","source":["## **test_create_model = 3 Marks** "],"metadata":{"id":"ArFxR31qNjmA"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":["create_model"],"id":"zjoEWfqc3kvj"},"outputs":[],"source":["def create_model(vocab_size, label_size, maxlen, embedding_dim, rnn_units):\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"YCfaRURB3kvk"},"outputs":[],"source":["model = create_model(vocab_size, label_size, maxlen, embedding_dim, rnn_units)\n","model.summary(line_length=90)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"pumghIqn3kvl"},"source":["Once the data has been processed and the model has been compiled, you can proceed to train it. \n","\n","You must complete the `train_model` function. The function takes as input the model created by `create_model`, the train input and output obtained by `format_examples` as well as the development input and output produced by the same function. The function also takes the `batch_size` and `epochs` hyperparameters. The function should train the model on the training data using those hyperparameters. During the training, `train_model` should evaluate the loss and any model metrics on the development data. With `shrink_dataset = False`, the training will take several minutes."]},{"cell_type":"markdown","source":["## **test_train_model= 4 Marks**"],"metadata":{"id":"kq3IeI6hNyK2"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":["train_model"],"id":"i5vqrOZG3kvl"},"outputs":[],"source":["def train_model(model, x_train, y_train, x_dev, y_dev, batch_size, epochs):\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"dwtu39OG3kvl"},"outputs":[],"source":["train_model(model, x_train, y_train, x_dev, y_dev, batch_size, epochs)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Ye99Vj8z3kvl"},"source":["After training, the model can be used to make predictions on unlabeled data using the [predict](https://keras.io/api/models/model_training_apis/#predict-method) method.\n","\n","You must complete the code for the `make_predictions` function. The functions takes as input the model already trained, the test input data produced by `format_examples` and the `batch_size` hyperparameter. The function must run the `predict` method on the input data using batches of size equal to `batch_size`. The `predict` method will return a **numpy** array with 3 axes: `(number of sentences, maxlen, label_size)`. For each token in each sentence, `predict` returns a vector with the probabilities predicted for every label. The output of `make_predictions` must include only the index of the label with the highest probability for each token. For example, if the prediction for one token is the vector `[0.04974193, 0.1511916, 0.65180656, 0.14725993]`, the output for that token should be `2`. For this, you can apply the [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) method along the last axis of the **numpy** array."]},{"cell_type":"markdown","source":["## **test_make_predictions= 3 Marks**"],"metadata":{"id":"-hGPoXqrOA0f"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":["make_predictions"],"id":"kiOJBgvV3kvl"},"outputs":[],"source":["def make_predictions(model, x_test, batch_size):\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"QU9FqsZk3kvm"},"outputs":[],"source":["predictions = make_predictions(model, x_test, batch_size)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"G_UrXssR3kvm"},"source":["Since the predictions are now label indexes, they can be translated to the corresponding label by accessing the `labels` list. The `predictions_to_labels` functions iterates over all the sequences in the test set and translates the prediction of each token to the corresponding label. The function skips the padding tokens. The new format of the predictions can be stored in the `prediction` column of the test `DataFrame`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EadjzW043kvm"},"outputs":[],"source":["def predictions_to_labels(predictions, x_test, labels):\n","    pred_labels = []\n","    for pred_seq, x_seq in zip(predictions, x_test):\n","        pred_seq_labels = [labels[p] for p, x in zip(pred_seq, x_seq) if x!=0]\n","        pred_labels.extend(pred_seq_labels)\n","    return pred_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"Ktc0FHDb3kvm"},"outputs":[],"source":["test_data['prediction'] = predictions_to_labels(predictions, x_test, labels)\n","test_data[(test_data.docId == \"S0038071711004354-1624\") & (test_data.sentId == 2)]"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"KwAXBX3b3kvn"},"source":["Although it is not the usual way of evaluating Sequence Labeling tasks, **MeasEval** uses a metric based on the Reading Comprehension *Macro-Averaged F1*. This metric measures the amount of overlapping tokens between the predictions and the true labels. In this assignment, we will approximate this metric by evaluating how many tokens belonging to a *Quantity* are captured by the model. This is done by the `evaluate` function. For the model trained above, the result of this evaluation should look like:\n","\n","> <pre>\n",">               precision    recall  f1-score   support\n","> \n",">     Quantity       0.85      0.69      0.76      1263\n","> \n",">    micro avg       0.85      0.69      0.76      1263\n",">    macro avg       0.85      0.69      0.76      1263\n","> weighted avg       0.85      0.69      0.76      1263\n","> </pre>"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"CvJda_FF3kvn"},"outputs":[],"source":["def evaluate(data):\n","    labels = data.apply(lambda x: x['label'].replace(\"B-\", \"\").replace(\"I-\", \"\"), axis=1).values\n","    predictions = data.apply(lambda x: x['prediction'].replace(\"B-\", \"\").replace(\"I-\", \"\"), axis=1).values\n","    print(classification_report(labels, predictions, labels=[\"Quantity\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"eEBSX8QO3kvn"},"outputs":[],"source":["evaluate(test_data)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Pb9NnwyK3kvn"},"source":["## Pre-trained Word Embeddings\n","\n","Initializing neural networks with pre-trained word embeddings has a significant impact on many NLP tasks. In the following exercise, you will experiment whether this is also the case for *Quantity* recognition using **GloVe**. You can refer to the following tutorial to learn how to complete this exercise with **keras**: [Using pre-trained word embeddings\n","](https://keras.io/examples/nlp/pretrained_word_embeddings/)\n","\n","\n","First, the `load_embeddings` function will load **GloVe** and return a dictionary mapping words to their embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"8l83XCCp3kvn"},"outputs":[],"source":["def load_embeddings(glove_path):\n","    embedding_index = {}\n","    with open(glove_path, encoding=\"utf8\") as glove_file:\n","        for line in glove_file:\n","            word, coefs = line.split(maxsplit=1)\n","            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","            embedding_index[word] = coefs\n","    return embedding_index"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"FIOPdTVS3kvo"},"outputs":[],"source":["glove_path = f\"glove/glove.6B.{embedding_dim}d.txt\"\n","embedding_index = load_embeddings(glove_path)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"OBCdc2YK3kvu"},"source":["To initialize the `Embedding` layer with the **GloVe** embeddings, you have to create a matrix with `(vocab_size, embedding_dim)` dimensions. The *i-th* row in the matrix corresponds to the *i-th* lemma in the vocabulary and contains the **GloVe** embedding for that lemma.\n","\n","You must complete the code for the `create_embedding_matrix` function. The function takes the embedding dictionary created by `load_embeddings`, the vocabulary dictionary, the size of the vocabulary and the `embedding_dim` hyperparameter. The function should initialize a `(vocab_size, embedding_dim)` **numpy** array with zeros and then replace each row with the appropriate **GloVe** embedding if the corresponding lemma exists in the embedding dictionary. For example, the embedding for \"*statistic*\" should exist in the resulting `embedding_matrix`: \n","> <pre>\n","array([ 0.1085    ,  0.82801998,  0.10672   ,  0.0094136 , -0.30441001,\n","        0.75617999, -0.14704999, -0.15469   , -0.97372001, -0.60413003,\n","        0.065233  , -0.055324  , -0.094477  ,  0.23502   ,  0.16466001,\n","        ...\n","</pre>"]},{"cell_type":"markdown","source":["### **test_create_embedding_matrix= 2 Marks**"],"metadata":{"id":"VyI4zA2aOSam"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":["create_embedding_matrix"],"id":"ENtaPzBj3kvu"},"outputs":[],"source":["def create_embedding_matrix(embedding_index, word2idx, vocab_size, embedding_dim):\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"BZTGA6Jh3kvv"},"outputs":[],"source":["embedding_matrix = create_embedding_matrix(embedding_index, word2idx, vocab_size, embedding_dim)\n","embedding_matrix[word2idx[\"statistic\"]]"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"gsfT7Ie83kvv"},"source":["Finally, you can create a new model and load the pre-trained word embedding matrix into the `Embedding` layer.\n","\n","You must complete the code for the `create_model_with_embeddings` function. This function takes as input the size of the vocabulary, the number of labels, the `maxlen`, `embedding_dim` and `rnn_units` hyperparameters, and the embedding matrix created by `create_embedding_matrix`. The function should construct, compile and return a RNN equal to the one built in `create_model` with the only difference that the `Embedding` layer must be initialized with the embedding matrix. Use the [Constant](https://keras.io/api/layers/initializers/) initializer for this purpose. For this task, the Embedding layer must be kept trainable so the embeddings can be updated during training.\n","\n","The configuration of the `Embedding` layer for this version of the RNN should look like:\n","> <pre>\n","> {'name': 'embedding_1',\n",">  'trainable': True,\n",">  'batch_input_shape': (None, 130),\n",">  'dtype': 'float32',\n",">  'input_dim': 5508,\n",">  'output_dim': 300,\n",">  'embeddings_initializer': {'class_name': 'Constant',\n",">   'config': {'value': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",">             0.        ,  0.        ],\n",">           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",">             0.        ,  0.        ],\n",">           [ 0.72004002,  0.80954999,  0.77170002, ...,  0.39351001,\n",">            -0.47082999, -0.60759002],\n",">           ...,\n",">           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",">             0.        ,  0.        ],\n",">           [-0.40123999, -0.27991   , -0.42445999, ...,  0.45576   ,\n",">             0.61864001, -0.30489001],\n",">           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",">             0.        ,  0.        ]])}},\n",">  'embeddings_regularizer': None,\n",">  'activity_regularizer': None,\n",">  'embeddings_constraint': None,\n",">  'mask_zero': True,\n",">  'input_length': 130}\n","> </pre>"]},{"cell_type":"markdown","source":["## **test_create_model_with_embeddings= 2 Marks**"],"metadata":{"id":"_TZ8gQY4Ob3J"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":["create_model_with_embeddings"],"id":"QsKN22gB3kvv"},"outputs":[],"source":["def create_model_with_embeddings(vocab_size, label_size, maxlen, embedding_dim, rnn_units, embedding_matrix):\n","    #\n","    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n","    #\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"SMvm0ZxV3kvw"},"outputs":[],"source":["model_with_embeddings = create_model_with_embeddings(vocab_size, label_size, maxlen, embedding_dim, rnn_units, embedding_matrix)\n","model_with_embeddings.get_layer(index=0).get_config()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"LHnOFGa53kvw"},"source":["Initializing the `Embedding` layer with **GloVe** embeddings should have a positive impact on the model performance for *Quantity* recognition by improving the recall.\n","\n","> <pre>\n",">               precision    recall  f1-score   support\n","> \n",">     Quantity       0.85      0.79      0.82      1263\n","> \n",">    micro avg       0.85      0.79      0.82      1263\n",">    macro avg       0.85      0.79      0.82      1263\n","> weighted avg       0.85      0.79      0.82      1263\n","> </pre>"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"ZsrQ8r8a3kvw"},"outputs":[],"source":["train_model(model_with_embeddings, x_train, y_train, x_dev, y_dev, batch_size, epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"7B-NyEcl3kvx"},"outputs":[],"source":["predictions_with_embeddings = make_predictions(model_with_embeddings, x_test, batch_size)\n","test_data['prediction'] = predictions_to_labels(predictions_with_embeddings, x_test, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"PWIvdfJq3kvx"},"outputs":[],"source":["evaluate(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9jT-0-63kvx"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
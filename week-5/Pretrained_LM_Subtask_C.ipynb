{"cells":[{"cell_type":"markdown","id":"824b3cd9-1766-4cb1-bd55-a8a21a3cbad5","metadata":{"deletable":false,"editable":false,"id":"zDBF1PXEDfhw"},"source":["# Pre-trained Language Models: SubTask C \n","## [8 Marks]\n","\n","In this assignment, you will work on the [ComVE](https://competitions.codalab.org/competitions/21080) shared task that was part of SemEval-2020. The task aims to evaluate whether a system can distinguish if a natural language statement makes sense to humans or not and provide a reason. **ConVE** includes three subtasks that require models to acquire and apply commonsense knowledge. In this notebook you will focus on **SubTask C**:\n","\n","- Given a statement that does not make sense, generate the reason why this statement does not make sense. For each nonsensical statement, three valid reasons are given as reference:\n","\n","     *Statement*: He put an elephant into the fridge.  \n","     *Reason A*: An elephant is much bigger than a fridge.  \n","     *Reason B*: A fridge is much smaller than an elephant.  \n","     *Reason C*: Most of the fridges aren't large enough to contain an elephant.\n","\n","     This subtask can be approached as a Sequence-to-Sequence problem where the input is the nonsensical statement and the output is a valid reason.\n","\n","You will fine-tune a Pre-trained Language Model with [Transformers](https://huggingface.co/docs/transformers/index) library that provides a set of tools for fine-tunning and deploying a wide variety of Pre-trained Language Models. The [Hugging Face Hub](https://huggingface.co/models) allows you to explore all the models supported by **Transformers** and even share your own models with the community. In this assignment, you will work with [BART](https://huggingface.co/docs/transformers/model_doc/bart), a pre-trained Sequence-to-Sequence model.\n","\n","Fine-tuning a Pre-trained Language Model usually requires a great amount of time and computational resources. Your personal computer will not be enough. In order to complete the assignment, you can work with a reduced version of the dataset and the base version of **BART**:"]},{"cell_type":"code","execution_count":1,"id":"0a297e57-7248-42ac-9e2c-710f9f70c99b","metadata":{"executionCancelledAt":null,"executionTime":1207,"id":"X6DavXXpDfhy","lastExecutedAt":1719732141242,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"shrink_dataset = True\nbase_model = True\ncolab = True"},"outputs":[],"source":["shrink_dataset = True\n","base_model = True\n","colab = True"]},{"cell_type":"code","execution_count":2,"id":"d3a8e32b-442b-4f2e-bc97-d108edd3208b","metadata":{"executionCancelledAt":null,"executionTime":109526,"lastExecutedAt":1719732250768,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"! pip install -r requirements.txt","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting ipython==8.5.0 (from -r requirements.txt (line 1))\n","  Downloading ipython-8.5.0-py3-none-any.whl.metadata (4.9 kB)\n","Collecting jupyter==1.0.0 (from -r requirements.txt (line 2))\n","  Downloading jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n","Collecting nbimporter==0.3.4 (from -r requirements.txt (line 3))\n","  Downloading nbimporter-0.3.4-py3-none-any.whl.metadata (252 bytes)\n","Collecting pytest==7.1.3 (from -r requirements.txt (line 4))\n","  Downloading pytest-7.1.3-py3-none-any.whl.metadata (7.7 kB)\n","Collecting pandas==1.3.5 (from -r requirements.txt (line 5))\n","  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting tensorflow==2.9.2 (from -r requirements.txt (line 6))\n","  Downloading tensorflow-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Collecting torch==2.0.0 (from -r requirements.txt (line 7))\n","  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n","Collecting transformers==4.23.1 (from -r requirements.txt (line 8))\n","  Downloading transformers-4.23.1-py3-none-any.whl.metadata (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets==2.11.0 (from -r requirements.txt (line 9))\n","  Downloading datasets-2.11.0-py3-none-any.whl.metadata (20 kB)\n","Collecting evaluate==0.4.0 (from -r requirements.txt (line 10))\n","  Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\n","Collecting rouge-score==0.1.2 (from -r requirements.txt (line 11))\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting backcall (from ipython==8.5.0->-r requirements.txt (line 1))\n","  Downloading backcall-0.2.0-py2.py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r requirements.txt (line 1)) (5.1.1)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r requirements.txt (line 1)) (0.19.1)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r requirements.txt (line 1)) (0.1.7)\n","Collecting pickleshare (from ipython==8.5.0->-r requirements.txt (line 1))\n","  Downloading pickleshare-0.7.5-py2.py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r requirements.txt (line 1)) (3.0.43)\n","Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r requirements.txt (line 1)) (2.18.0)\n","Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r requirements.txt (line 1)) (0.6.3)\n","Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r requirements.txt (line 1)) (5.14.3)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r requirements.txt (line 1)) (4.9.0)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 2)) (7.1.3)\n","Collecting qtconsole (from jupyter==1.0.0->-r requirements.txt (line 2))\n","  Downloading qtconsole-5.5.2-py3-none-any.whl.metadata (5.1 kB)\n","Collecting jupyter-console (from jupyter==1.0.0->-r requirements.txt (line 2))\n","  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 2)) (7.16.4)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 2)) (6.29.4)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 2)) (8.1.2)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.3->-r requirements.txt (line 4)) (23.2.0)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.3->-r requirements.txt (line 4)) (2.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.3->-r requirements.txt (line 4)) (23.2)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.3->-r requirements.txt (line 4)) (1.5.0)\n","Collecting py>=1.8.2 (from pytest==7.1.3->-r requirements.txt (line 4))\n","  Downloading py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.3->-r requirements.txt (line 4)) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5->-r requirements.txt (line 5)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5->-r requirements.txt (line 5)) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5->-r requirements.txt (line 5)) (1.26.4)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (2.1.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (1.6.3)\n","Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.2->-r requirements.txt (line 6))\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl.metadata (872 bytes)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.2->-r requirements.txt (line 6))\n","  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (1.63.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (3.11.0)\n","Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.2->-r requirements.txt (line 6))\n","  Downloading keras-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.2->-r requirements.txt (line 6))\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (3.3.0)\n","Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.9.2->-r requirements.txt (line 6))\n","  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (69.5.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (1.16.0)\n","Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.2->-r requirements.txt (line 6))\n","  Downloading tensorboard-2.9.1-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (0.37.0)\n","Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.2->-r requirements.txt (line 6))\n","  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (4.11.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r requirements.txt (line 6)) (1.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 7)) (3.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 7)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 7)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 7)) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.0.0 (from torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1->-r requirements.txt (line 8)) (0.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1->-r requirements.txt (line 8)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1->-r requirements.txt (line 8)) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1->-r requirements.txt (line 8)) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.23.1->-r requirements.txt (line 8))\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1->-r requirements.txt (line 8)) (4.66.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.11.0->-r requirements.txt (line 9)) (15.0.2)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets==2.11.0->-r requirements.txt (line 9))\n","  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.11.0->-r requirements.txt (line 9)) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.11.0->-r requirements.txt (line 9)) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.11.0->-r requirements.txt (line 9)) (2024.3.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.11.0->-r requirements.txt (line 9)) (3.9.5)\n","Collecting responses<0.19 (from datasets==2.11.0->-r requirements.txt (line 9))\n","  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.1.2->-r requirements.txt (line 11)) (3.8.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r requirements.txt (line 7)) (0.43.0)\n","Collecting cmake (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading cmake-3.29.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n","Collecting lit (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 7))\n","  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.11.0->-r requirements.txt (line 9)) (1.3.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.11.0->-r requirements.txt (line 9)) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.11.0->-r requirements.txt (line 9)) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.11.0->-r requirements.txt (line 9)) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.11.0->-r requirements.txt (line 9)) (4.0.3)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==8.5.0->-r requirements.txt (line 1)) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==8.5.0->-r requirements.txt (line 1)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython==8.5.0->-r requirements.txt (line 1)) (0.2.13)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1->-r requirements.txt (line 8)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1->-r requirements.txt (line 8)) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1->-r requirements.txt (line 8)) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1->-r requirements.txt (line 8)) (2024.2.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r requirements.txt (line 6)) (2.29.0)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r requirements.txt (line 6))\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r requirements.txt (line 6)) (3.6)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r requirements.txt (line 6))\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n","Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r requirements.txt (line 6))\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r requirements.txt (line 6)) (3.0.3)\n","Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (0.2.2)\n","Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (1.8.1)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (8.6.1)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (5.7.2)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (1.6.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (5.9.8)\n","Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (26.0.3)\n","Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (6.4)\n","Requirement already satisfied: widgetsnbextension~=4.0.10 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 2)) (4.0.10)\n","Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 2)) (3.0.10)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->-r requirements.txt (line 7)) (2.1.5)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Collecting multiprocess (from datasets==2.11.0->-r requirements.txt (line 9))\n","  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (4.12.3)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (6.1.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (0.7.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (0.3.0)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (3.0.2)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (0.10.0)\n","Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (5.7.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (1.5.1)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score==0.1.2->-r requirements.txt (line 11)) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score==0.1.2->-r requirements.txt (line 11)) (1.4.2)\n","Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (2.14.1)\n","Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (2.25.4)\n","Requirement already satisfied: jupyterlab<4.2,>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (4.1.6)\n","Requirement already satisfied: notebook-shim<0.3,>=0.2 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.2.4)\n","Collecting qtpy>=2.4.0 (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 2))\n","  Downloading QtPy-2.4.1-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython==8.5.0->-r requirements.txt (line 1)) (2.0.1)\n","Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython==8.5.0->-r requirements.txt (line 1)) (2.4.1)\n","Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython==8.5.0->-r requirements.txt (line 1)) (0.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->-r requirements.txt (line 7)) (1.3.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (0.5.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r requirements.txt (line 6)) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r requirements.txt (line 6)) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r requirements.txt (line 6)) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r requirements.txt (line 6)) (2.0.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (4.2.2)\n","Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (4.3.0)\n","Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (23.1.0)\n","Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.10.0)\n","Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.5.3)\n","Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (7.7.0)\n","Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.20.0)\n","Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.18.1)\n","Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (1.8.0)\n","Requirement already satisfied: async-lru>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (2.0.4)\n","Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.27.0)\n","Requirement already satisfied: jupyter-lsp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (2.2.5)\n","Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (2.15.0)\n","Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.9.25)\n","Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (4.22.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (2.19.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (2.5)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (1.3.1)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (1.2.1)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (21.2.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.14.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.18.1)\n","Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (2.0.7)\n","Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.1.4)\n","Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.1.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r requirements.txt (line 6)) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r requirements.txt (line 6)) (3.2.2)\n","Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (1.5.1)\n","Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (20.11.0)\n","Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (2.4)\n","Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (1.13)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (2.22)\n","Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (2.9.0.20240316)\n","Downloading ipython-8.5.0-py3-none-any.whl (752 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.0/752.0 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n","Downloading nbimporter-0.3.4-py3-none-any.whl (4.9 kB)\n","Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tensorflow-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading datasets-2.11.0-py3-none-any.whl (468 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n","Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n","Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n","Downloading qtconsole-5.5.2-py3-none-any.whl (123 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cmake-3.29.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=6a5d04ae7b66e7d002b28b756ba155607ad65b50284e3d29a7166f9161480827\n","  Stored in directory: /home/repl/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge-score\n","Installing collected packages: tokenizers, tensorboard-plugin-wit, pickleshare, nbimporter, lit, keras, flatbuffers, backcall, tensorflow-estimator, tensorboard-data-server, qtpy, py, protobuf, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, keras-preprocessing, gast, dill, cmake, rouge-score, responses, pytest, pandas, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, transformers, ipython, google-auth-oauthlib, tensorboard, datasets, tensorflow, qtconsole, jupyter-console, evaluate, jupyter, triton, torch\n","\u001b[33m  WARNING: The script lit is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script qtpy is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The scripts cmake, cpack and ctest are installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The scripts py.test and pytest are installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The scripts ipython and ipython3 are installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script tensorboard is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script datasets-cli is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script jupyter-console is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script evaluate-cli is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cohere 5.5.8 requires tokenizers<1,>=0.15, but you have tokenizers 0.13.3 which is incompatible.\n","dbt-core 1.7.10 requires protobuf<5,>=4.0.0, but you have protobuf 3.19.6 which is incompatible.\n","geopandas 0.14.4 requires pandas>=1.4.0, but you have pandas 1.3.5 which is incompatible.\n","grpcio-status 1.62.2 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n","mapclassify 2.6.1 requires pandas!=1.5.0,>=1.4, but you have pandas 1.3.5 which is incompatible.\n","mizani 0.11.3 requires pandas>=2.1.0, but you have pandas 1.3.5 which is incompatible.\n","pingouin 0.5.4 requires pandas>=1.5, but you have pandas 1.3.5 which is incompatible.\n","plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.3.5 which is incompatible.\n","pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.3.5 which is incompatible.\n","sentence-transformers 2.7.0 requires transformers<5.0.0,>=4.34.0, but you have transformers 4.23.1 which is incompatible.\n","statsmodels 0.14.2 requires pandas!=2.1.0,>=1.4, but you have pandas 1.3.5 which is incompatible.\n","tensorflow-cpu 2.16.1 requires flatbuffers>=23.5.26, but you have flatbuffers 1.12 which is incompatible.\n","tensorflow-cpu 2.16.1 requires keras>=3.0.0, but you have keras 2.9.0 which is incompatible.\n","tensorflow-cpu 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-cpu 2.16.1 requires tensorboard<2.17,>=2.16, but you have tensorboard 2.9.1 which is incompatible.\n","visions 0.7.6 requires pandas>=2.0.0, but you have pandas 1.3.5 which is incompatible.\n","web3 6.18.0 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n","xarray 2024.5.0 requires pandas>=2.0, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed backcall-0.2.0 cmake-3.29.6 datasets-2.11.0 dill-0.3.6 evaluate-0.4.0 flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 ipython-8.5.0 jupyter-1.0.0 jupyter-console-6.6.3 keras-2.9.0 keras-preprocessing-1.1.2 lit-18.1.8 multiprocess-0.70.14 nbimporter-0.3.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pandas-1.3.5 pickleshare-0.7.5 protobuf-3.19.6 py-1.11.0 pytest-7.1.3 qtconsole-5.5.2 qtpy-2.4.1 responses-0.18.0 rouge-score-0.1.2 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.2 tensorflow-estimator-2.9.0 tokenizers-0.13.3 torch-2.0.0 transformers-4.23.1 triton-2.0.0\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"]}],"source":["! pip install -r requirements.txt"]},{"cell_type":"markdown","id":"254e7add-6dcb-4129-8056-212189f7efc8","metadata":{"deletable":false,"editable":false,"id":"fI5Uu0_gDfhz"},"source":["Although the value of these variables do not affect the tests that will evaluate your code, the output examples distributed throughout this notebook are based on a `shrink_dataset` and a `base_model` variables set as `True`, and a `colab` variable set as `False`.\n","\n","If you want to perform a full training of the model to obtain its real performance, you can use a cloud service like [Google Colab](https://colab.research.google.com/). **Colab** is a **Jupyter** notebook environment that supports both GPU and TPU instances, allowing training large scale Deep Learning models. Set the `shrink_dataset` and a `base_model` variables to `False`, the `colab` variable to `True`, and follow the instructions provided to you to run the notebook in **Colab**."]},{"cell_type":"code","execution_count":3,"id":"83231a28-38e6-4c41-bd0c-43439b59f9bd","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":8482,"id":"gEqfRl12Dfhz","lastExecutedAt":1719732259252,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"if colab:\n    ! pip install transformers datasets evaluate\n    import os\n    if not os.path.exists(\"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskC_data_all.csv\"):\n        ! git clone https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation.git SemEval2020-Task4-Data","outputsMetadata":{"0":{"height":616,"type":"stream"},"1":{"height":616,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: transformers in /home/repl/.local/lib/python3.10/site-packages (4.23.1)\n","Requirement already satisfied: datasets in /home/repl/.local/lib/python3.10/site-packages (2.11.0)\n","Requirement already satisfied: evaluate in /home/repl/.local/lib/python3.10/site-packages (0.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/repl/.local/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (15.0.2)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/repl/.local/lib/python3.10/site-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pandas in /home/repl/.local/lib/python3.10/site-packages (from datasets) (1.3.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /home/repl/.local/lib/python3.10/site-packages (from datasets) (0.70.14)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2024.3.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: responses<0.19 in /home/repl/.local/lib/python3.10/site-packages (from datasets) (0.18.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"]}],"source":["if colab:\n","    ! pip install transformers datasets evaluate\n","    import os\n","    if not os.path.exists(\"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskC_data_all.csv\"):\n","        ! git clone https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation.git SemEval2020-Task4-Data"]},{"cell_type":"markdown","id":"8ac7deea-4e10-4e4f-a7fc-0194f60fb689","metadata":{"deletable":false,"editable":false,"id":"U1c3v4XVDfhz"},"source":["You will use the following objects and functions:"]},{"cell_type":"code","execution_count":4,"id":"3daa6b34-4d1a-4f76-8e75-1eb2c1c2523b","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":8080,"id":"Nef4cdirDfhz","lastExecutedAt":1719732267334,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport evaluate\nimport tqdm as notebook_tqdm\nfrom datasets import Dataset\nfrom transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, \n                          Seq2SeqTrainingArguments, Seq2SeqTrainer, \n                          DataCollatorForSeq2Seq, enable_full_determinism)","outputsMetadata":{"0":{"height":101,"type":"stream"}}},"outputs":[],"source":["import pandas as pd\n","import evaluate\n","import tqdm as notebook_tqdm\n","from datasets import Dataset\n","from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, \n","                          Seq2SeqTrainingArguments, Seq2SeqTrainer, \n","                          DataCollatorForSeq2Seq, enable_full_determinism)"]},{"cell_type":"markdown","id":"670981f0-0378-4b86-99d7-6afa1fa1d3a7","metadata":{"deletable":false,"editable":false,"id":"rjm8g6k8Dfhz"},"source":["When working with Neural Networks, there are a large number of random operations such as initializing the weights of the network, shuffling the data for training, or choosing samples. This causes that different training runs of the same model can lead to different results. To ensure reproducibility, i.e. obtaining the same results in the different runs, the random number generator must be initialized with a fixed value known as seed. In Transformers, this can be done as follows:"]},{"cell_type":"code","execution_count":5,"id":"6b034844-5d39-4969-940b-ede6c691a90d","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":55,"id":"9GC6UlXcDfh0","lastExecutedAt":1719732267390,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"enable_full_determinism(seed=42)"},"outputs":[],"source":["enable_full_determinism(seed=42)"]},{"cell_type":"markdown","id":"976ae958-2723-4e8d-b6f6-3d8042d1b4be","metadata":{"deletable":false,"editable":false,"id":"J8U_rnboDfh0"},"source":["> **Note!** With models as complex as Neural Networks, reproducibility is susceptible to factors such as software versions or the hardware on which the models are run. Even with seed initialization, there may be slight differences in the results.\n","\n","Working with Neural Networks also involves defining a number of hyperparameters that set the configuration of the model. Finding the appropriate hyperparameter values requires training the model with different combinations and testing them on the development set. This hyperparameter tuning is a costly process that needs multiple rounds of experimentation. However, for this assignments, you will use the following values:"]},{"cell_type":"code","execution_count":6,"id":"1974b262-7f39-436d-a42d-45da2bad11e8","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":51,"id":"8eBiASC4Dfh0","lastExecutedAt":1719732267442,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"epochs = 3  # Number of epochs to train the model\ntrain_batch_size = 8  # Number of examples used per gradient update\nlearning_rate = 1e-5  # The learning rate for the optimizer\nmax_length = 25  # Maximum lenght of the input sequence\noutput_dir = \"modelC\"  # The output directory where the model will be written"},"outputs":[],"source":["epochs = 3  # Number of epochs to train the model\n","train_batch_size = 8  # Number of examples used per gradient update\n","learning_rate = 1e-5  # The learning rate for the optimizer\n","max_length = 25  # Maximum lenght of the input sequence\n","output_dir = \"modelC\"  # The output directory where the model will be written"]},{"cell_type":"markdown","id":"d7c9efed-8e29-404d-9116-d33b3c552592","metadata":{"deletable":false,"editable":false,"id":"eAd8K6wXDfh0"},"source":["> **Note!** The notebook for this assignment provides very little guidance. You are expected to refer to the [documentation](https://huggingface.co/docs) for details on how to solve the exercises."]},{"cell_type":"markdown","id":"d75fbe2a-808e-4778-b4d9-ddfa68eeb11a","metadata":{"deletable":false,"editable":false,"id":"kGbzR7H8Dfh0"},"source":["## Loading the Pre-trained Model - [1 Mark]\n","\n","The first step you must perform in this assignment is to load the model and its corresponding tokenizer using the classes imported above."]},{"cell_type":"code","execution_count":7,"id":"46a728ea-182d-478d-8ceb-8cea3a34a128","metadata":{"executionCancelledAt":null,"executionTime":48,"id":"Yk7tOsLaDfh0","lastExecutedAt":1719732267490,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def load_model(model_name):   # [1 Mark]\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n    return model, tokenizer"},"outputs":[],"source":["def load_model(model_name):   # [1 Mark]\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":8,"id":"4aef17a5-3959-46ce-9998-cf825628c475","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":3444,"id":"lEVPC3rlDfh0","lastExecutedAt":1719732270935,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"model_name = \"facebook/bart-base\" if base_model else \"facebook/bart-large\"\nmodel, tokenizer = load_model(model_name)","outputsMetadata":{"0":{"height":143,"type":"stream"},"5":{"height":122,"type":"stream"}}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc4ef06c91144ebebd6dff68f3f531f5","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79683091b14342c48cd7958e0651bf64","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a2ddca47b7041e0b0c05de64a2ea713","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28706b15047942d2af3f3819ed3a59c0","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0f86deaaec84feaaf27caaf5ff74179","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model_name = \"facebook/bart-base\" if base_model else \"facebook/bart-large\"\n","model, tokenizer = load_model(model_name)"]},{"cell_type":"markdown","id":"eadcbcc2-7886-4ea1-ae56-2509b5125cee","metadata":{"deletable":false,"editable":false,"id":"-q7CcBksDfh1"},"source":["## Data Pre-processing - [2 Marks]\n","\n","The **ComVE** dataset consists of 10000 nonsensical statements for the train set, 997 statements for development and 1000 for test. Each nonsensical statements comes with with three reference valid reasons. You must load the three sets into three `DataFrames`. For the training and development splits, the `DataFrame` should contain three columns: the `id` of the nonsensical statement, a `FalseSent` column with the nonsensical statement and a `reason` column with the reference reasons. For the test set, the `DataFrame` should contain five columns: the `id` of the nonsensical statement, a `FalseSent` column with the nonsensical statement and three columns (`reason1`, `reason2` and `reason3`) containing each of the reference reasons.\n","\n","Train DataFrame:\n","\n","|       |   id | FalseSent                                         | reason                                                                         |\n","|------:|-----:|:--------------------------------------------------|:-------------------------------------------------------------------------------|\n","|   769 |  769 | Computers is an ingredient used in preparing food | Computers are not used for food and they are not edible                        |\n","| 10769 |  769 | Computers is an ingredient used in preparing food | Computer is not something that can be used in preparing food.                  |\n","| 20769 |  769 | Computers is an ingredient used in preparing food | You cannot eat a computer                                                      |\n","|   888 |  888 | he did hear music in his cooling glass            | cooling glass can not play the song, it's not a electronic thing to play music |\n","| 10888 |  888 | he did hear music in his cooling glass            | Glass does not produce music.                                                  |\n","| 20888 |  888 | he did hear music in his cooling glass            | Any sound that might be made by a cooling glass is not music.                  |\n","\n","Test DataFrame:\n","\n","|     |   id | FalseSent                                      | reason1                                                  | reason2                                                | reason3                                                            |\n","|----:|-----:|:-----------------------------------------------|:---------------------------------------------------------|:-------------------------------------------------------|:-------------------------------------------------------------------|\n","|  76 | 1280 | Beer that is drunk by humans is white          | Beer is made of barley and it is a yellow drink          | A beer that is drunk by humans is not white.           | Beer is brown                                                      |\n","| 101 |  860 | eating trash food every day makes you stronger | eating trash food every day makes your body fat and weak | eating trash food every day is bad for your health     | Trash food could be contaminated                                   |\n","| 136 |  777 | he put some cooking oil in his wine            | cooking oil will destroy the taste of the wine           | Cooking oil does not go in wine                        | Cooking oil does not taste nice and therefore would ruin the wine. |\n","| 174 |  570 | Lobsters live in the mountains                 | Lobsters needs water to live                             | Lobsters live in the sea.                              | Lobsters live in the sea, not the mountains                        |\n","| 210 | 1929 | the clock shows animals                        | the clock is used to show the time to people             | Clocks are required to tell the time, not show animals | a clock shows the time not animals                                 |\n","| 235 | 1619 | she put the giraffe in the freezer             | A giraffe is much bigger than the freezer                | There is no way a giraffe is fitting in the freezer.   | A giraffe is too big to be put in a freezer.                       |"]},{"cell_type":"code","execution_count":9,"id":"bf4feb50-5acc-4f97-9ac4-0f31c49225b2","metadata":{"executionCancelledAt":null,"executionTime":722,"id":"GVeX5et-Dfh1","lastExecutedAt":1719732271659,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def load_data(data_csv, answers_csv, is_test=False):\n    data_df = pd.read_csv(data_csv)\n    answers_df = pd.read_csv(answers_csv, header=None, names=['id', 'reason1', 'reason2', 'reason3'])\n\n    if 'id' not in data_df.columns or 'FalseSent' not in data_df.columns:\n        raise KeyError(\"Invalid csv, The main data CSV must contain 'id' and 'FalseSent' columns.\")\n\n    if is_test:\n        merged_df = data_df.merge(answers_df, on='id')\n        return merged_df[['id', 'FalseSent', 'reason1', 'reason2', 'reason3']]\n    else:\n        answers_melted = answers_df.melt(id_vars=['id'], \n                                         value_vars=['reason1', 'reason2', 'reason3'],\n                                         var_name='reason_type', value_name='reason').drop(columns=['reason_type'])\n        # Merge data and melted answers on 'id'\n        merged_df = data_df.merge(answers_melted, on='id')\n        return merged_df[['id', 'FalseSent', 'reason']]"},"outputs":[],"source":["def load_data(data_csv, answers_csv, is_test=False):\n","    data_df = pd.read_csv(data_csv)\n","    answers_df = pd.read_csv(answers_csv, header=None, names=['id', 'reason1', 'reason2', 'reason3'])\n","\n","    if 'id' not in data_df.columns or 'FalseSent' not in data_df.columns:\n","        raise KeyError(\"Invalid csv, The main data CSV must contain 'id' and 'FalseSent' columns.\")\n","\n","    if is_test:\n","        merged_df = data_df.merge(answers_df, on='id')\n","        return merged_df[['id', 'FalseSent', 'reason1', 'reason2', 'reason3']]\n","    else:\n","        answers_melted = answers_df.melt(id_vars=['id'], \n","                                         value_vars=['reason1', 'reason2', 'reason3'],\n","                                         var_name='reason_type', value_name='reason').drop(columns=['reason_type'])\n","        # Merge data and melted answers on 'id'\n","        merged_df = data_df.merge(answers_melted, on='id')\n","        return merged_df[['id', 'FalseSent', 'reason']]"]},{"cell_type":"code","execution_count":10,"id":"d899505f-1452-458e-88a7-ef86bb8ecb62","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":178,"id":"tE231iQzDfh1","lastExecutedAt":1719732271837,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"train_data_csv = \"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskC_data_all.csv\"\ntrain_answers_csv = \"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskC_answers_all.csv\"\ntrain_data = load_data(train_data_csv, train_answers_csv)\ndev_data_csv = \"SemEval2020-Task4-Data/ALL data/Dev Data/subtaskC_dev_data.csv\"\ndev_answers_csv = \"SemEval2020-Task4-Data/ALL data/Dev Data/subtaskC_gold_answers.csv\"\ndev_data = load_data(dev_data_csv, dev_answers_csv)\ntest_data_csv = \"SemEval2020-Task4-Data/ALL data/Test Data/subtaskC_test_data.csv\"\ntest_answers_csv = \"SemEval2020-Task4-Data/ALL data/Test Data/subtaskC_gold_answers.csv\"\ntest_data = load_data(test_data_csv, test_answers_csv, True)\nif shrink_dataset:\n    idxs = train_data[\"id\"].sample(frac=1, random_state=42).unique()[:30]\n    train_data = train_data[train_data.id.isin(idxs)]\n    idxs = dev_data[\"id\"].sample(frac=1, random_state=42).unique()[:30]\n    dev_data = dev_data[dev_data.id.isin(idxs)]\n    idxs = test_data[\"id\"].sample(frac=1, random_state=42).unique()[:30]\n    test_data = test_data[test_data.id.isin(idxs)]\npd.set_option(\"display.max_colwidth\", None)\nprint(\"Train DataFrame:\")\ndisplay(train_data[:6])\nprint(\"Test DataFrame:\")\ndisplay(test_data[:6])","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":221,"type":"dataFrame"},"2":{"height":38,"type":"stream"},"3":{"height":221,"type":"dataFrame"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Train DataFrame:\n"]},{"data":{"application/com.datacamp.data-table.v2+json":{"table":{"data":{"FalseSent":["Computers is an ingredient used in preparing food","Computers is an ingredient used in preparing food","Computers is an ingredient used in preparing food","he did hear music in his cooling glass","he did hear music in his cooling glass","he did hear music in his cooling glass"],"id":[769,769,769,888,888,888],"index":[2307,2308,2309,2664,2665,2666],"reason":["Computers are not used for food and they are not edible","Computer is not something that can be used in preparing food.","You cannot eat a computer","cooling glass can not play the song, it's not a electronic thing to play music","Glass does not produce music.","Any sound that might be made by a cooling glass is not music."]},"schema":{"fields":[{"name":"index","type":"integer"},{"name":"id","type":"integer"},{"name":"FalseSent","type":"string"},{"name":"reason","type":"string"}],"pandas_version":"1.4.0","primaryKey":["index"]}},"total_rows":6,"truncation_type":null},"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>FalseSent</th>\n","      <th>reason</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2307</th>\n","      <td>769</td>\n","      <td>Computers is an ingredient used in preparing food</td>\n","      <td>Computers are not used for food and they are not edible</td>\n","    </tr>\n","    <tr>\n","      <th>2308</th>\n","      <td>769</td>\n","      <td>Computers is an ingredient used in preparing food</td>\n","      <td>Computer is not something that can be used in preparing food.</td>\n","    </tr>\n","    <tr>\n","      <th>2309</th>\n","      <td>769</td>\n","      <td>Computers is an ingredient used in preparing food</td>\n","      <td>You cannot eat a computer</td>\n","    </tr>\n","    <tr>\n","      <th>2664</th>\n","      <td>888</td>\n","      <td>he did hear music in his cooling glass</td>\n","      <td>cooling glass can not play the song, it's not a electronic thing to play music</td>\n","    </tr>\n","    <tr>\n","      <th>2665</th>\n","      <td>888</td>\n","      <td>he did hear music in his cooling glass</td>\n","      <td>Glass does not produce music.</td>\n","    </tr>\n","    <tr>\n","      <th>2666</th>\n","      <td>888</td>\n","      <td>he did hear music in his cooling glass</td>\n","      <td>Any sound that might be made by a cooling glass is not music.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id  ...                                                                          reason\n","2307  769  ...                         Computers are not used for food and they are not edible\n","2308  769  ...                   Computer is not something that can be used in preparing food.\n","2309  769  ...                                                       You cannot eat a computer\n","2664  888  ...  cooling glass can not play the song, it's not a electronic thing to play music\n","2665  888  ...                                                   Glass does not produce music.\n","2666  888  ...                   Any sound that might be made by a cooling glass is not music.\n","\n","[6 rows x 3 columns]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test DataFrame:\n"]},{"data":{"application/com.datacamp.data-table.v2+json":{"table":{"data":{"FalseSent":["Beer that is drunk by humans is white","eating trash food every day makes you stronger","he put some cooking oil in his wine","Lobsters live in the mountains","the clock shows animals","she put the giraffe in the freezer"],"id":[1280,860,777,570,1929,1619],"index":[76,101,136,174,210,235],"reason1":["Beer is made of barley and it is a yellow drink","eating trash food every day makes your body fat and weak","cooking oil will destroy the taste of the wine","Lobsters needs water to live","the clock is used to show the time to people","A giraffe is much bigger than the freezer"],"reason2":["A beer that is drunk by humans is not white.","eating trash food every day is bad for your health","Cooking oil does not go in wine","Lobsters live in the sea.","Clocks are required to tell the time, not show animals","There is no way a giraffe is fitting in the freezer."],"reason3":["Beer is brown","Trash food could be contaminated","Cooking oil does not taste nice and therefore would ruin the wine.","Lobsters live in the sea, not the mountains","a clock shows the time not animals","A giraffe is too big to be put in a freezer."]},"schema":{"fields":[{"name":"index","type":"integer"},{"name":"id","type":"integer"},{"name":"FalseSent","type":"string"},{"name":"reason1","type":"string"},{"name":"reason2","type":"string"},{"name":"reason3","type":"string"}],"pandas_version":"1.4.0","primaryKey":["index"]}},"total_rows":6,"truncation_type":null},"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>FalseSent</th>\n","      <th>reason1</th>\n","      <th>reason2</th>\n","      <th>reason3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>76</th>\n","      <td>1280</td>\n","      <td>Beer that is drunk by humans is white</td>\n","      <td>Beer is made of barley and it is a yellow drink</td>\n","      <td>A beer that is drunk by humans is not white.</td>\n","      <td>Beer is brown</td>\n","    </tr>\n","    <tr>\n","      <th>101</th>\n","      <td>860</td>\n","      <td>eating trash food every day makes you stronger</td>\n","      <td>eating trash food every day makes your body fat and weak</td>\n","      <td>eating trash food every day is bad for your health</td>\n","      <td>Trash food could be contaminated</td>\n","    </tr>\n","    <tr>\n","      <th>136</th>\n","      <td>777</td>\n","      <td>he put some cooking oil in his wine</td>\n","      <td>cooking oil will destroy the taste of the wine</td>\n","      <td>Cooking oil does not go in wine</td>\n","      <td>Cooking oil does not taste nice and therefore would ruin the wine.</td>\n","    </tr>\n","    <tr>\n","      <th>174</th>\n","      <td>570</td>\n","      <td>Lobsters live in the mountains</td>\n","      <td>Lobsters needs water to live</td>\n","      <td>Lobsters live in the sea.</td>\n","      <td>Lobsters live in the sea, not the mountains</td>\n","    </tr>\n","    <tr>\n","      <th>210</th>\n","      <td>1929</td>\n","      <td>the clock shows animals</td>\n","      <td>the clock is used to show the time to people</td>\n","      <td>Clocks are required to tell the time, not show animals</td>\n","      <td>a clock shows the time not animals</td>\n","    </tr>\n","    <tr>\n","      <th>235</th>\n","      <td>1619</td>\n","      <td>she put the giraffe in the freezer</td>\n","      <td>A giraffe is much bigger than the freezer</td>\n","      <td>There is no way a giraffe is fitting in the freezer.</td>\n","      <td>A giraffe is too big to be put in a freezer.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id  ...                                                             reason3\n","76   1280  ...                                                       Beer is brown\n","101   860  ...                                    Trash food could be contaminated\n","136   777  ...  Cooking oil does not taste nice and therefore would ruin the wine.\n","174   570  ...                         Lobsters live in the sea, not the mountains\n","210  1929  ...                                  a clock shows the time not animals\n","235  1619  ...                        A giraffe is too big to be put in a freezer.\n","\n","[6 rows x 5 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["train_data_csv = \"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskC_data_all.csv\"\n","train_answers_csv = \"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskC_answers_all.csv\"\n","train_data = load_data(train_data_csv, train_answers_csv)\n","dev_data_csv = \"SemEval2020-Task4-Data/ALL data/Dev Data/subtaskC_dev_data.csv\"\n","dev_answers_csv = \"SemEval2020-Task4-Data/ALL data/Dev Data/subtaskC_gold_answers.csv\"\n","dev_data = load_data(dev_data_csv, dev_answers_csv)\n","test_data_csv = \"SemEval2020-Task4-Data/ALL data/Test Data/subtaskC_test_data.csv\"\n","test_answers_csv = \"SemEval2020-Task4-Data/ALL data/Test Data/subtaskC_gold_answers.csv\"\n","test_data = load_data(test_data_csv, test_answers_csv, True)\n","if shrink_dataset:\n","    idxs = train_data[\"id\"].sample(frac=1, random_state=42).unique()[:30]\n","    train_data = train_data[train_data.id.isin(idxs)]\n","    idxs = dev_data[\"id\"].sample(frac=1, random_state=42).unique()[:30]\n","    dev_data = dev_data[dev_data.id.isin(idxs)]\n","    idxs = test_data[\"id\"].sample(frac=1, random_state=42).unique()[:30]\n","    test_data = test_data[test_data.id.isin(idxs)]\n","pd.set_option(\"display.max_colwidth\", None)\n","print(\"Train DataFrame:\")\n","display(train_data[:6])\n","print(\"Test DataFrame:\")\n","display(test_data[:6])"]},{"cell_type":"code","execution_count":11,"id":"9fd31c3e-3647-4992-ad04-592580eb32f0","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":50,"id":"DJU-01EtDfh1","lastExecutedAt":1719732271887,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"train_dataset = Dataset.from_pandas(train_data)\ndev_dataset = Dataset.from_pandas(dev_data)\ntest_dataset = Dataset.from_pandas(test_data)\nprint(\"Train Dataset example:\")\ndisplay(train_dataset[0])\nprint(\"Test Dataset example:\")\ndisplay(test_dataset[0])","outputsMetadata":{"0":{"height":38,"type":"stream"},"2":{"height":38,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Dataset example:\n"]},{"data":{"text/plain":["{'id': 769,\n"," 'FalseSent': 'Computers is an ingredient used in preparing food',\n"," 'reason': 'Computers are not used for food and they are not edible',\n"," '__index_level_0__': 2307}"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test Dataset example:\n"]},{"data":{"text/plain":["{'id': 1280,\n"," 'FalseSent': 'Beer that is drunk by humans is white',\n"," 'reason1': 'Beer is made of barley and it is a yellow drink',\n"," 'reason2': 'A beer that is drunk by humans is not white.',\n"," 'reason3': 'Beer is brown',\n"," '__index_level_0__': 76}"]},"metadata":{},"output_type":"display_data"}],"source":["train_dataset = Dataset.from_pandas(train_data)\n","dev_dataset = Dataset.from_pandas(dev_data)\n","test_dataset = Dataset.from_pandas(test_data)\n","print(\"Train Dataset example:\")\n","display(train_dataset[0])\n","print(\"Test Dataset example:\")\n","display(test_dataset[0])"]},{"cell_type":"markdown","id":"698e389b-fc3f-4802-96d6-475aad6eb2cc","metadata":{"deletable":false,"editable":false,"id":"HsEcHmGiDfh1"},"source":["The `Datasets` should be pre-processed following two different approaches. \n","\n","- For the test `Dataset`, you must run the tokenizer on the `FalseSent` column and store the result in the `input_ids` and `attention_mask` fields. \n","\n","- For the train and development `Datasets` you must also run the tokenizer on the `reason` column and store the resulting `input_ids` in the `labels` field. In all cases, the tokenizer must pad and truncate the sequences to the `max_length` value.\n","\n","><pre>\n",">Train formated Dataset example:\n",">\n",">{'id': 769, 'FalseSent': 'Computers is an ingredient used in preparing food', 'reason': 'Computers are not used for food and they are not edible', '__index_level_0__': 769, 'input_ids': [0, 14721, 43990, 16, 41, 16181, 341, 11, 4568, 689, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 14721, 43990, 32, 45, 341, 13, 689, 8, 51, 32, 45, 27532, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",">\n",">Test formated Dataset example:\n",">\n",">{'id': 1280, 'FalseSent': 'Beer that is drunk by humans is white', 'reason1': 'Beer is made of barley and it is a yellow drink', 'reason2': 'A beer that is drunk by humans is not white.', 'reason3': 'Beer is brown', '__index_level_0__': 76, 'input_ids': [0, 45562, 14, 16, 10789, 30, 5868, 16, 1104, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","></pre>"]},{"cell_type":"code","execution_count":12,"id":"ef857d46-cbfa-422b-9725-eb7744e12aed","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1719732271943,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import platform\nplatform.platform()"},"outputs":[{"data":{"text/plain":["'Linux-5.10.216-204.855.amzn2.x86_64-x86_64-with-glibc2.35'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import platform\n","platform.platform()"]},{"cell_type":"code","execution_count":13,"id":"767c1aa8-ed63-42ad-96f1-65c61e3a1b73","metadata":{"executionCancelledAt":null,"executionTime":51,"id":"Y17HBOk0Dfh1","lastExecutedAt":1719732271994,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def preprocess_data(examples, tokenizer, max_length, is_test=False):\n    # Tokenize the FalseSent column\n    inputs = tokenizer(\n        examples[\"FalseSent\"],\n        max_length=max_length,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    \n    # Initialize the result dictionary\n    result = {\n        \"input_ids\": inputs[\"input_ids\"],\n        \"attention_mask\": inputs[\"attention_mask\"]\n    }\n    \n    if not is_test:\n        # Tokenize the reason column and add it to the result as labels\n        labels = tokenizer(\n            examples[\"reason\"],\n            max_length=max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        result[\"labels\"] = labels[\"input_ids\"]\n    \n    return result"},"outputs":[],"source":["def preprocess_data(examples, tokenizer, max_length, is_test=False):\n","    # Tokenize the FalseSent column\n","    inputs = tokenizer(\n","        examples[\"FalseSent\"],\n","        max_length=max_length,\n","        padding=\"max_length\",\n","        truncation=True,\n","        return_tensors=\"pt\"\n","    )\n","    \n","    # Initialize the result dictionary\n","    result = {\n","        \"input_ids\": inputs[\"input_ids\"],\n","        \"attention_mask\": inputs[\"attention_mask\"]\n","    }\n","    \n","    if not is_test:\n","        # Tokenize the reason column and add it to the result as labels\n","        labels = tokenizer(\n","            examples[\"reason\"],\n","            max_length=max_length,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )\n","        result[\"labels\"] = labels[\"input_ids\"]\n","    \n","    return result"]},{"cell_type":"code","execution_count":14,"id":"09e8f6fe-8aa7-4ee7-ae53-835fe2555480","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":215,"id":"TYDVjG1SDfh1","lastExecutedAt":1719732272209,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"train_dataset = train_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\ndev_dataset = dev_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\ntest_dataset = test_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length, True), batched=True)\nprint(\"Train formated Dataset example:\\n\")\nprint(train_dataset[0])\nprint(\"\\nTest formated Dataset example:\\n\")\nprint(test_dataset[0])","outputsMetadata":{"3":{"height":395,"type":"stream"}}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24a159ed7c5a49b78e7352e14faad164","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/90 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e9b128499e144f889db587071497283","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/90 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"359cca912a93467d8a35e1df8e488ab1","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/30 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Train formated Dataset example:\n","\n","{'id': 769, 'FalseSent': 'Computers is an ingredient used in preparing food', 'reason': 'Computers are not used for food and they are not edible', '__index_level_0__': 2307, 'input_ids': [0, 14721, 43990, 16, 41, 16181, 341, 11, 4568, 689, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 14721, 43990, 32, 45, 341, 13, 689, 8, 51, 32, 45, 27532, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","\n","Test formated Dataset example:\n","\n","{'id': 1280, 'FalseSent': 'Beer that is drunk by humans is white', 'reason1': 'Beer is made of barley and it is a yellow drink', 'reason2': 'A beer that is drunk by humans is not white.', 'reason3': 'Beer is brown', '__index_level_0__': 76, 'input_ids': [0, 45562, 14, 16, 10789, 30, 5868, 16, 1104, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]}],"source":["train_dataset = train_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\n","dev_dataset = dev_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\n","test_dataset = test_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length, True), batched=True)\n","print(\"Train formated Dataset example:\\n\")\n","print(train_dataset[0])\n","print(\"\\nTest formated Dataset example:\\n\")\n","print(test_dataset[0])"]},{"cell_type":"markdown","id":"6453dd3c-d6d7-46bc-bb2f-13c831ed2104","metadata":{"deletable":false,"editable":false,"id":"SE65PJbXDfh2"},"source":["## Fine-tuning - [5 Marks]\n","\n","In general, when using a `Trainer` to make predictions, it returns the logits for each class in the task. However, the `Seq2SeqTrainingArguments` class provides an option that allows the `Trainer` to generate sequences of tokens in the prediction. The `create_training_arguments` function must create the `Seq2SeqTrainingArguments` with that option and the hyperparamters passed as arguments. During the training, the model must be evaluated on the development set after every epoch. `TrainingArguments` should include this strategy.\n","\n","> **Important!** By default, `Trainer` saves a checkpoint of the model every 500 training steps. For this assignment, avoid this behavior by setting `save_strategy=\"no\"` when creating the `TrainingArguments`."]},{"cell_type":"code","execution_count":15,"id":"65190c6f-965b-4007-bb9d-c6dc00a11864","metadata":{"executionCancelledAt":null,"executionTime":150,"id":"NYcGf9LtDfh2","lastExecutedAt":1719732272359,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def create_training_arguments(epochs, train_batch_size, learning_rate, output_dir):   # [1 Mark]\n    seq_2_seq_training_args = Seq2SeqTrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=epochs,\n        per_device_train_batch_size=train_batch_size,\n        evaluation_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        save_strategy=\"no\",\n        metric_for_best_model=\"accuracy\",\n        predict_with_generate=True\n    )\n\n    return seq_2_seq_training_args"},"outputs":[],"source":["def create_training_arguments(epochs, train_batch_size, learning_rate, output_dir):   # [1 Mark]\n","    seq_2_seq_training_args = Seq2SeqTrainingArguments(\n","        output_dir=output_dir,\n","        num_train_epochs=epochs,\n","        per_device_train_batch_size=train_batch_size,\n","        evaluation_strategy=\"epoch\",\n","        learning_rate=learning_rate,\n","        save_strategy=\"no\",\n","        metric_for_best_model=\"accuracy\",\n","        predict_with_generate=True\n","    )\n","\n","    return seq_2_seq_training_args"]},{"cell_type":"code","execution_count":16,"id":"ab63610b-93f6-4b0f-8552-7aaa4d0added","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":51,"id":"OMIWd-JsDfh2","lastExecutedAt":1719732272410,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"train_args = create_training_arguments(epochs, train_batch_size, learning_rate, output_dir)","outputsMetadata":{"0":{"height":101,"type":"stream"}}},"outputs":[],"source":["train_args = create_training_arguments(epochs, train_batch_size, learning_rate, output_dir)"]},{"cell_type":"markdown","id":"766e1e45-0e95-4e87-9af7-3fb76427dead","metadata":{"deletable":false,"editable":false,"id":"LlaJ4yJxDfh2"},"source":["Next, you can create a `Trainer` object initializing the appropriate data collator."]},{"cell_type":"code","execution_count":17,"id":"cda9aca4-71d2-4d86-9fb1-a1246312ae96","metadata":{"executionCancelledAt":null,"executionTime":48,"id":"b3Zqw2vpDfh2","lastExecutedAt":1719732272458,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def create_trainer(model, train_args, train_dataset, dev_dataset, tokenizer):   # [1 Mark]\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        padding=True,\n        model=model\n    )\n    \n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=train_args,\n        train_dataset=train_dataset,\n        eval_dataset=dev_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator\n    )\n\n    return trainer"},"outputs":[],"source":["def create_trainer(model, train_args, train_dataset, dev_dataset, tokenizer):   # [1 Mark]\n","    data_collator = DataCollatorForSeq2Seq(\n","        tokenizer=tokenizer,\n","        padding=True,\n","        model=model\n","    )\n","    \n","    trainer = Seq2SeqTrainer(\n","        model=model,\n","        args=train_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=dev_dataset,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator\n","    )\n","\n","    return trainer"]},{"cell_type":"code","execution_count":18,"id":"82c5b2cb-2a03-4946-96ef-34a5a595d172","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":927,"id":"3E2txek_Dfh2","lastExecutedAt":1719732273386,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"trainer = create_trainer(model, train_args, train_dataset, dev_dataset, tokenizer)"},"outputs":[],"source":["trainer = create_trainer(model, train_args, train_dataset, dev_dataset, tokenizer)"]},{"cell_type":"code","execution_count":19,"id":"59c84010-e41a-4c5c-a8a2-3dcdab530418","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":46170,"id":"wtQpnSvdDfh2","lastExecutedAt":1719732319557,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"trainer.train()","outputsMetadata":{"0":{"height":311,"type":"stream"},"2":{"height":563,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: __index_level_0__, reason, id, FalseSent. If __index_level_0__, reason, id, FalseSent are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 90\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 36\n","You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 2/36 : < :, Epoch 0.08/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: __index_level_0__, reason, id, FalseSent. If __index_level_0__, reason, id, FalseSent are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: __index_level_0__, reason, id, FalseSent. If __index_level_0__, reason, id, FalseSent are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: __index_level_0__, reason, id, FalseSent. If __index_level_0__, reason, id, FalseSent are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=36, training_loss=8.944275750054253, metrics={'train_runtime': 45.9206, 'train_samples_per_second': 5.88, 'train_steps_per_second': 0.784, 'total_flos': 4019258880000.0, 'train_loss': 8.944275750054253, 'epoch': 3.0})"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","id":"2352f1c7-c39a-4a66-8567-cdc327c6f174","metadata":{"deletable":false,"editable":false,"id":"e7Dq4Z3QDfh2"},"source":["If you have set the `Seq2SeqTrainingArguments` properly, you could now use the `Trainer` to predict sequences of tokens. Take into account that `Trainer` will return the indexes of the tokens, so the sequence must be decoded to obtain the text strings. The `tokenizer` provides functionality to do this. The result of this process can be stored in the `prediction` column of the test `DataFrame`:\n","\n","|     |   id | FalseSent                                      | reason1                                                  | reason2                                                | reason3                                                            | prediction                                     |\n","|----:|-----:|:-----------------------------------------------|:---------------------------------------------------------|:-------------------------------------------------------|:-------------------------------------------------------------------|:-----------------------------------------------|\n","|  76 | 1280 | Beer that is drunk by humans is white          | Beer is made of barley and it is a yellow drink          | A beer that is drunk by humans is not white.           | Beer is brown                                                      | Beer that is drunk by humans is white                             |\n","| 101 |  860 | eating trash food every day makes you stronger | eating trash food every day makes your body fat and weak | eating trash food every day is bad for your health     | Trash food could be contaminated                                   | eating trash food every day makes you stronger |\n","| 136 |  777 | he put some cooking oil in his wine            | cooking oil will destroy the taste of the wine           | Cooking oil does not go in wine                        | Cooking oil does not taste nice and therefore would ruin the wine. | he put some cooking oil in his wine            |\n","| 174 |  570 | Lobsters live in the mountains                 | Lobsters needs water to live                             | Lobsters live in the sea.                              | Lobsters live in the sea, not the mountains                        | Lobsters live in mountains                 |\n","| 210 | 1929 | the clock shows animals                        | the clock is used to show the time to people             | Clocks are required to tell the time, not show animals | a clock shows the time not animals                                 | the clock shows animals                        |\n","| 235 | 1619 | she put the giraffe in the freezer             | A giraffe is much bigger than the freezer                | There is no way a giraffe is fitting in the freezer.   | A giraffe is too big to be put in a freezer.                       | she put the giraffe in the freezer             |\n"]},{"cell_type":"code","execution_count":22,"id":"0128dd6c-6d26-4d35-b24f-07d552c258fc","metadata":{"executionCancelledAt":null,"executionTime":12,"id":"8igEq42FDfh2","lastExecutedAt":1719732375101,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\n\ndef make_predictions(trainer, test_dataset, tokenizer):\n    \"\"\"\n    Use the trained model to predict the reasons for why the given nonsensical statements\n    do not make sense, using the Seq2SeqTrainer.\n\n    Parameters:\n    - trainer (Seq2SeqTrainer): The trained model.\n    - test_dataset (Dataset): The dataset containing the nonsensical statements, prepared for prediction.\n    - tokenizer (AutoTokenizer): Tokenizer for decoding the predictions.\n\n    Returns:\n    - pd.DataFrame: A DataFrame with the original test data and the predictions.\n    \"\"\"\n    # Generate predictions using the Trainer\n    test_predictions = trainer.predict(test_dataset)\n    test_predictions_list = test_predictions.predictions\n    predictions_decoded = tokenizer.batch_decode(test_predictions_list, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n\n    return predictions_decoded\n"},"outputs":[],"source":["import pandas as pd\n","\n","def make_predictions(trainer, test_dataset, tokenizer):\n","    \"\"\"\n","    Use the trained model to predict the reasons for why the given nonsensical statements\n","    do not make sense, using the Seq2SeqTrainer.\n","\n","    Parameters:\n","    - trainer (Seq2SeqTrainer): The trained model.\n","    - test_dataset (Dataset): The dataset containing the nonsensical statements, prepared for prediction.\n","    - tokenizer (AutoTokenizer): Tokenizer for decoding the predictions.\n","\n","    Returns:\n","    - pd.DataFrame: A DataFrame with the original test data and the predictions.\n","    \"\"\"\n","    # Generate predictions using the Trainer\n","    test_predictions = trainer.predict(test_dataset)\n","    test_predictions_list = test_predictions.predictions\n","    predictions_decoded = tokenizer.batch_decode(test_predictions_list, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","\n","    return predictions_decoded\n"]},{"cell_type":"code","execution_count":23,"id":"589d71b2-ad31-46f4-a2ac-48ea518a64e2","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":4458,"id":"zaptZPfJDfh2","lastExecutedAt":1719732387635,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Updated code\npredictions = make_predictions(trainer, test_dataset, tokenizer)\ntest_data[\"prediction\"] = predictions\ntest_data\n","outputsMetadata":{"0":{"height":185,"type":"stream"},"1":{"height":616,"type":"stream"},"2":{"columns":{"prediction":{"wrap":true}},"height":701,"type":"dataFrame"},"3":{"height":321,"type":"dataFrame"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: reason2, reason3, reason1, id, FalseSent, __index_level_0__. If reason2, reason3, reason1, id, FalseSent, __index_level_0__ are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 30\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/4 : < :]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/com.datacamp.data-table.v2+json":{"table":{"data":{"FalseSent":["Beer that is drunk by humans is white","eating trash food every day makes you stronger","he put some cooking oil in his wine","Lobsters live in the mountains","the clock shows animals","she put the giraffe in the freezer","he installed the carpet on the lake","My son had us write an essay on The National Monument.","He drove up the stairs to the bedroom","he put a piece of plastic on the bread","My mom always cuts my fish's hair.","She put the filing cabinet into the papers.","Grizzly bears hate honey.","the baby held a grizzly bear","She throws a stove in the hat.","fishing nets are useful for covering a window","Pens are for painting","My window speaks very well.","The lion used the litter box","Cigarette is good for healthy","Mike ran four sandwiches.","The cat likes to watch alligators.","The supermarket only sells cars","The man ate the bowl when he was hungry.","a flower petal is part of a motor vehicle","Renting movies is the newest trend","People are usually green","beepers are becoming even more popular","the sun rises in the west","at night it's easy to find sun"],"id":[1280,860,777,570,1929,1619,979,75,1810,774,1739,324,6,901,343,1044,207,443,1455,13,308,1439,1696,1548,863,745,1883,1030,986,905],"index":[76,101,136,174,210,235,280,319,371,411,513,521,527,549,626,636,660,678,737,740,761,811,859,883,899,902,938,947,973,986],"prediction":["Beer that is drunk by humans is white","eating trash food every day makes you stronger","he put some cooking oil in his wine","Lobsters live in mountains","the clock shows animals","she put the giraffe in the freezer","he installed the carpet on the lake","My son had us write an essay on The National Monument.","He drove up the stairs to the bedroom","he put a piece of plastic on the bread","My mom always cuts my fish's hair.","She put the filing cabinet into the papers.","Grizzly bears hate honey.","the baby held a grizzly bear","She throws a stove in the hat.","fishing nets are useful for covering a window","Pens are for painting","My window speaks very well.","The lion used the litter box","Cigarette is good for healthy","Mike ran four sandwiches.","The cat likes to watch alligators.","The supermarket only sells cars","The man ate the bowl when he was hungry.","a flower petal is part of a motor vehicle","Renting movies is the newest trend","People are usually green","beepers are becoming even more popular","the sun rises in the west","at night it's easy to find sun"],"reason1":["Beer is made of barley and it is a yellow drink","eating trash food every day makes your body fat and weak","cooking oil will destroy the taste of the wine","Lobsters needs water to live","the clock is used to show the time to people","A giraffe is much bigger than the freezer","The carpet will absorb water and sink","My son isn't smart enough to assign an essay.","A car is too large to fit upstairs","the plastic usually is toxic","Fish don't have any hair.","Nothing can be put into the paper.","Honey is good for grizzly bear's growth","a baby would be eaten by a grizzly bear","A stove is bigger than a hat.","fishing nets have holes in them that leave gaps of a window uncovered","Pens are a writing utensil","Your window cannot speak because it is an object.","A domestic cat is tame and use litter boxes","Lung will be damaged by smoking cigarette","You can't run a sandwich.","Cats and alligators lives in different areas in the world","They have a variety of goods and services in the supermarket, mainly household supplies","People cannot eat bowls.","a flower petal isn't a motor component","Fewer and fewer people like renting movies","No one is born green","no one uses beepers anymore","the sun rises in the east and sets in the west","sun can be seen only during the day"],"reason2":["A beer that is drunk by humans is not white.","eating trash food every day is bad for your health","Cooking oil does not go in wine","Lobsters live in the sea.","Clocks are required to tell the time, not show animals","There is no way a giraffe is fitting in the freezer.","Carpets need a subfloor.","My son is studying in the seconds standard only","Stairs are too small and weak for a car to drive up","You can't eat plastic.","Fish have scales instead of hair like we have","it doesnt fit into papers","Grizzly bears have been observed in the wild seeking out honey to eat","Baby can hold a teddy bear but not grizzly bear.","stove is too big to fit in a hat","Fishing net will smell so bad and look so unclean on a window.","Pens are not brushes.","windows dont talk","The lion is too big for a litter box","Cigarettes contain carcinogens.","A sandwich is not a distance","Cats like to watch things they can eat.","A supermarket implies the sale of smaller goods like groceries or toiletries.","The bowl is not edible.","A motor vehicle does not contain flower parts","Renting has been around a long time.","People are not green.","The beeper trend died in the 90s.","It depends on the earth position","By definition night is after the sun has gone"],"reason3":["Beer is brown","Trash food could be contaminated","Cooking oil does not taste nice and therefore would ruin the wine.","Lobsters live in the sea, not the mountains","a clock shows the time not animals","A giraffe is too big to be put in a freezer.","A lake would not be able to grip onto carpet.","Children don't ask parents to write essays.","People don't drive up indoor stairs.","People do not eat plastic because it's not a food","Fish don't have hair.","The filing cabinet is what papers go into.","Grizzly bears love honey.","A grizzly bear is bigger than a baby and would eat a baby.","Stove cannot be thrown in the hat","Nets don't block out light.","A pen is not made for painting.","a window does not have a voice","litter boxes are only for cats","Cigarettes cause cancer.","Sandwich is food and not need to run for it.","The cat likes to watch birds and not alligators.","Supermarkets don't sell cars.","One cannot eat a bowl","Petals aren't car parts.","Renting movies is outdated and for something to be trending it must be popular","No existing human race has green skin.","Beepers have gone out of style with all of the new available cell phones, so they are rarely used anymore.","The sun always sets in the west","The sun is not out at night"]},"schema":{"fields":[{"name":"index","type":"integer"},{"name":"id","type":"integer"},{"name":"FalseSent","type":"string"},{"name":"reason1","type":"string"},{"name":"reason2","type":"string"},{"name":"reason3","type":"string"},{"name":"prediction","type":"string"}],"pandas_version":"1.4.0","primaryKey":["index"]}},"total_rows":30,"truncation_type":null},"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>FalseSent</th>\n","      <th>reason1</th>\n","      <th>reason2</th>\n","      <th>reason3</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>76</th>\n","      <td>1280</td>\n","      <td>Beer that is drunk by humans is white</td>\n","      <td>Beer is made of barley and it is a yellow drink</td>\n","      <td>A beer that is drunk by humans is not white.</td>\n","      <td>Beer is brown</td>\n","      <td>Beer that is drunk by humans is white</td>\n","    </tr>\n","    <tr>\n","      <th>101</th>\n","      <td>860</td>\n","      <td>eating trash food every day makes you stronger</td>\n","      <td>eating trash food every day makes your body fat and weak</td>\n","      <td>eating trash food every day is bad for your health</td>\n","      <td>Trash food could be contaminated</td>\n","      <td>eating trash food every day makes you stronger</td>\n","    </tr>\n","    <tr>\n","      <th>136</th>\n","      <td>777</td>\n","      <td>he put some cooking oil in his wine</td>\n","      <td>cooking oil will destroy the taste of the wine</td>\n","      <td>Cooking oil does not go in wine</td>\n","      <td>Cooking oil does not taste nice and therefore would ruin the wine.</td>\n","      <td>he put some cooking oil in his wine</td>\n","    </tr>\n","    <tr>\n","      <th>174</th>\n","      <td>570</td>\n","      <td>Lobsters live in the mountains</td>\n","      <td>Lobsters needs water to live</td>\n","      <td>Lobsters live in the sea.</td>\n","      <td>Lobsters live in the sea, not the mountains</td>\n","      <td>Lobsters live in mountains</td>\n","    </tr>\n","    <tr>\n","      <th>210</th>\n","      <td>1929</td>\n","      <td>the clock shows animals</td>\n","      <td>the clock is used to show the time to people</td>\n","      <td>Clocks are required to tell the time, not show animals</td>\n","      <td>a clock shows the time not animals</td>\n","      <td>the clock shows animals</td>\n","    </tr>\n","    <tr>\n","      <th>235</th>\n","      <td>1619</td>\n","      <td>she put the giraffe in the freezer</td>\n","      <td>A giraffe is much bigger than the freezer</td>\n","      <td>There is no way a giraffe is fitting in the freezer.</td>\n","      <td>A giraffe is too big to be put in a freezer.</td>\n","      <td>she put the giraffe in the freezer</td>\n","    </tr>\n","    <tr>\n","      <th>280</th>\n","      <td>979</td>\n","      <td>he installed the carpet on the lake</td>\n","      <td>The carpet will absorb water and sink</td>\n","      <td>Carpets need a subfloor.</td>\n","      <td>A lake would not be able to grip onto carpet.</td>\n","      <td>he installed the carpet on the lake</td>\n","    </tr>\n","    <tr>\n","      <th>319</th>\n","      <td>75</td>\n","      <td>My son had us write an essay on The National Monument.</td>\n","      <td>My son isn't smart enough to assign an essay.</td>\n","      <td>My son is studying in the seconds standard only</td>\n","      <td>Children don't ask parents to write essays.</td>\n","      <td>My son had us write an essay on The National Monument.</td>\n","    </tr>\n","    <tr>\n","      <th>371</th>\n","      <td>1810</td>\n","      <td>He drove up the stairs to the bedroom</td>\n","      <td>A car is too large to fit upstairs</td>\n","      <td>Stairs are too small and weak for a car to drive up</td>\n","      <td>People don't drive up indoor stairs.</td>\n","      <td>He drove up the stairs to the bedroom</td>\n","    </tr>\n","    <tr>\n","      <th>411</th>\n","      <td>774</td>\n","      <td>he put a piece of plastic on the bread</td>\n","      <td>the plastic usually is toxic</td>\n","      <td>You can't eat plastic.</td>\n","      <td>People do not eat plastic because it's not a food</td>\n","      <td>he put a piece of plastic on the bread</td>\n","    </tr>\n","    <tr>\n","      <th>513</th>\n","      <td>1739</td>\n","      <td>My mom always cuts my fish's hair.</td>\n","      <td>Fish don't have any hair.</td>\n","      <td>Fish have scales instead of hair like we have</td>\n","      <td>Fish don't have hair.</td>\n","      <td>My mom always cuts my fish's hair.</td>\n","    </tr>\n","    <tr>\n","      <th>521</th>\n","      <td>324</td>\n","      <td>She put the filing cabinet into the papers.</td>\n","      <td>Nothing can be put into the paper.</td>\n","      <td>it doesnt fit into papers</td>\n","      <td>The filing cabinet is what papers go into.</td>\n","      <td>She put the filing cabinet into the papers.</td>\n","    </tr>\n","    <tr>\n","      <th>527</th>\n","      <td>6</td>\n","      <td>Grizzly bears hate honey.</td>\n","      <td>Honey is good for grizzly bear's growth</td>\n","      <td>Grizzly bears have been observed in the wild seeking out honey to eat</td>\n","      <td>Grizzly bears love honey.</td>\n","      <td>Grizzly bears hate honey.</td>\n","    </tr>\n","    <tr>\n","      <th>549</th>\n","      <td>901</td>\n","      <td>the baby held a grizzly bear</td>\n","      <td>a baby would be eaten by a grizzly bear</td>\n","      <td>Baby can hold a teddy bear but not grizzly bear.</td>\n","      <td>A grizzly bear is bigger than a baby and would eat a baby.</td>\n","      <td>the baby held a grizzly bear</td>\n","    </tr>\n","    <tr>\n","      <th>626</th>\n","      <td>343</td>\n","      <td>She throws a stove in the hat.</td>\n","      <td>A stove is bigger than a hat.</td>\n","      <td>stove is too big to fit in a hat</td>\n","      <td>Stove cannot be thrown in the hat</td>\n","      <td>She throws a stove in the hat.</td>\n","    </tr>\n","    <tr>\n","      <th>636</th>\n","      <td>1044</td>\n","      <td>fishing nets are useful for covering a window</td>\n","      <td>fishing nets have holes in them that leave gaps of a window uncovered</td>\n","      <td>Fishing net will smell so bad and look so unclean on a window.</td>\n","      <td>Nets don't block out light.</td>\n","      <td>fishing nets are useful for covering a window</td>\n","    </tr>\n","    <tr>\n","      <th>660</th>\n","      <td>207</td>\n","      <td>Pens are for painting</td>\n","      <td>Pens are a writing utensil</td>\n","      <td>Pens are not brushes.</td>\n","      <td>A pen is not made for painting.</td>\n","      <td>Pens are for painting</td>\n","    </tr>\n","    <tr>\n","      <th>678</th>\n","      <td>443</td>\n","      <td>My window speaks very well.</td>\n","      <td>Your window cannot speak because it is an object.</td>\n","      <td>windows dont talk</td>\n","      <td>a window does not have a voice</td>\n","      <td>My window speaks very well.</td>\n","    </tr>\n","    <tr>\n","      <th>737</th>\n","      <td>1455</td>\n","      <td>The lion used the litter box</td>\n","      <td>A domestic cat is tame and use litter boxes</td>\n","      <td>The lion is too big for a litter box</td>\n","      <td>litter boxes are only for cats</td>\n","      <td>The lion used the litter box</td>\n","    </tr>\n","    <tr>\n","      <th>740</th>\n","      <td>13</td>\n","      <td>Cigarette is good for healthy</td>\n","      <td>Lung will be damaged by smoking cigarette</td>\n","      <td>Cigarettes contain carcinogens.</td>\n","      <td>Cigarettes cause cancer.</td>\n","      <td>Cigarette is good for healthy</td>\n","    </tr>\n","    <tr>\n","      <th>761</th>\n","      <td>308</td>\n","      <td>Mike ran four sandwiches.</td>\n","      <td>You can't run a sandwich.</td>\n","      <td>A sandwich is not a distance</td>\n","      <td>Sandwich is food and not need to run for it.</td>\n","      <td>Mike ran four sandwiches.</td>\n","    </tr>\n","    <tr>\n","      <th>811</th>\n","      <td>1439</td>\n","      <td>The cat likes to watch alligators.</td>\n","      <td>Cats and alligators lives in different areas in the world</td>\n","      <td>Cats like to watch things they can eat.</td>\n","      <td>The cat likes to watch birds and not alligators.</td>\n","      <td>The cat likes to watch alligators.</td>\n","    </tr>\n","    <tr>\n","      <th>859</th>\n","      <td>1696</td>\n","      <td>The supermarket only sells cars</td>\n","      <td>They have a variety of goods and services in the supermarket, mainly household supplies</td>\n","      <td>A supermarket implies the sale of smaller goods like groceries or toiletries.</td>\n","      <td>Supermarkets don't sell cars.</td>\n","      <td>The supermarket only sells cars</td>\n","    </tr>\n","    <tr>\n","      <th>883</th>\n","      <td>1548</td>\n","      <td>The man ate the bowl when he was hungry.</td>\n","      <td>People cannot eat bowls.</td>\n","      <td>The bowl is not edible.</td>\n","      <td>One cannot eat a bowl</td>\n","      <td>The man ate the bowl when he was hungry.</td>\n","    </tr>\n","    <tr>\n","      <th>899</th>\n","      <td>863</td>\n","      <td>a flower petal is part of a motor vehicle</td>\n","      <td>a flower petal isn't a motor component</td>\n","      <td>A motor vehicle does not contain flower parts</td>\n","      <td>Petals aren't car parts.</td>\n","      <td>a flower petal is part of a motor vehicle</td>\n","    </tr>\n","    <tr>\n","      <th>902</th>\n","      <td>745</td>\n","      <td>Renting movies is the newest trend</td>\n","      <td>Fewer and fewer people like renting movies</td>\n","      <td>Renting has been around a long time.</td>\n","      <td>Renting movies is outdated and for something to be trending it must be popular</td>\n","      <td>Renting movies is the newest trend</td>\n","    </tr>\n","    <tr>\n","      <th>938</th>\n","      <td>1883</td>\n","      <td>People are usually green</td>\n","      <td>No one is born green</td>\n","      <td>People are not green.</td>\n","      <td>No existing human race has green skin.</td>\n","      <td>People are usually green</td>\n","    </tr>\n","    <tr>\n","      <th>947</th>\n","      <td>1030</td>\n","      <td>beepers are becoming even more popular</td>\n","      <td>no one uses beepers anymore</td>\n","      <td>The beeper trend died in the 90s.</td>\n","      <td>Beepers have gone out of style with all of the new available cell phones, so they are rarely used anymore.</td>\n","      <td>beepers are becoming even more popular</td>\n","    </tr>\n","    <tr>\n","      <th>973</th>\n","      <td>986</td>\n","      <td>the sun rises in the west</td>\n","      <td>the sun rises in the east and sets in the west</td>\n","      <td>It depends on the earth position</td>\n","      <td>The sun always sets in the west</td>\n","      <td>the sun rises in the west</td>\n","    </tr>\n","    <tr>\n","      <th>986</th>\n","      <td>905</td>\n","      <td>at night it's easy to find sun</td>\n","      <td>sun can be seen only during the day</td>\n","      <td>By definition night is after the sun has gone</td>\n","      <td>The sun is not out at night</td>\n","      <td>at night it's easy to find sun</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id  ...                                              prediction\n","76   1280  ...                   Beer that is drunk by humans is white\n","101   860  ...          eating trash food every day makes you stronger\n","136   777  ...                     he put some cooking oil in his wine\n","174   570  ...                              Lobsters live in mountains\n","210  1929  ...                                 the clock shows animals\n","235  1619  ...                      she put the giraffe in the freezer\n","280   979  ...                     he installed the carpet on the lake\n","319    75  ...  My son had us write an essay on The National Monument.\n","371  1810  ...                   He drove up the stairs to the bedroom\n","411   774  ...                  he put a piece of plastic on the bread\n","513  1739  ...                      My mom always cuts my fish's hair.\n","521   324  ...             She put the filing cabinet into the papers.\n","527     6  ...                               Grizzly bears hate honey.\n","549   901  ...                            the baby held a grizzly bear\n","626   343  ...                          She throws a stove in the hat.\n","636  1044  ...           fishing nets are useful for covering a window\n","660   207  ...                                   Pens are for painting\n","678   443  ...                             My window speaks very well.\n","737  1455  ...                            The lion used the litter box\n","740    13  ...                           Cigarette is good for healthy\n","761   308  ...                               Mike ran four sandwiches.\n","811  1439  ...                      The cat likes to watch alligators.\n","859  1696  ...                         The supermarket only sells cars\n","883  1548  ...                The man ate the bowl when he was hungry.\n","899   863  ...               a flower petal is part of a motor vehicle\n","902   745  ...                      Renting movies is the newest trend\n","938  1883  ...                                People are usually green\n","947  1030  ...                  beepers are becoming even more popular\n","973   986  ...                               the sun rises in the west\n","986   905  ...                          at night it's easy to find sun\n","\n","[30 rows x 6 columns]"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Updated code\n","predictions = make_predictions(trainer, test_dataset, tokenizer)\n","test_data[\"prediction\"] = predictions\n","test_data\n"]},{"cell_type":"markdown","id":"e8c8a535-71d1-4f17-aa3d-c17d08434140","metadata":{"deletable":false,"editable":false,"id":"2gAv_hl0Dfh3"},"source":["The **Subtasks B** of **ComVE** is evaluated using the *bleu* metric. In this assignment, you will also evaluate using *rouge*. With `shrink_dataset` and `base_model` set to `True`, the expected scores are *0.216* and *0.446* for *bleu* and *rouge* respectively. With a full training run, i.e. with `shrink_dataset` and `base_model` set to `False`, the scores should be around *0.228* and *0.461*."]},{"cell_type":"code","execution_count":69,"id":"9d73f93a-c57c-41d1-a284-863a79d8e61e","metadata":{"executionCancelledAt":null,"executionTime":52,"id":"yDJ79cPoDfh3","lastExecutedAt":1719734508974,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def evaluate_prediction(test_data: pd.DataFrame, metric: str) -> float:\n    \"\"\"\n    Evaluates predictions in a Pandas DataFrame using the specified metric.\n\n    Args:\n        test_data (pd.DataFrame): DataFrame containing test data with columns\n            for predictions and ground truth (e.g., \"reason1\", \"reason2\", \"reason3\").\n        metric (str): Name of the metric from the `evaluate` library.\n\n    Returns:\n        float: The computed metric score.\n    \"\"\"\n\n    # Extract references (ground truth) from specific columns\n    reference = test_data[[\"reason1\", \"reason2\", \"reason3\"]].values.tolist()\n\n    # Extract predictions from a dedicated column\n    predictions = test_data[\"prediction\"].tolist()\n\n    # Load the specified metric from the `evaluate` library\n    metric = evaluate.load(metric)\n\n    # Compute the metric score using predictions and references\n    return metric.compute(predictions=predictions, references=reference)\n"},"outputs":[],"source":["def evaluate_prediction(test_data: pd.DataFrame, metric: str) -> float:\n","    \"\"\"\n","    Evaluates predictions in a Pandas DataFrame using the specified metric.\n","\n","    Args:\n","        test_data (pd.DataFrame): DataFrame containing test data with columns\n","            for predictions and ground truth (e.g., \"reason1\", \"reason2\", \"reason3\").\n","        metric (str): Name of the metric from the `evaluate` library.\n","\n","    Returns:\n","        float: The computed metric score.\n","    \"\"\"\n","\n","    # Extract references (ground truth) from specific columns\n","    reference = test_data[[\"reason1\", \"reason2\", \"reason3\"]].values.tolist()\n","\n","    # Extract predictions from a dedicated column\n","    predictions = test_data[\"prediction\"].tolist()\n","\n","    # Load the specified metric from the `evaluate` library\n","    metric = evaluate.load(metric)\n","\n","    # Compute the metric score using predictions and references\n","    return metric.compute(predictions=predictions, references=reference)\n"]},{"cell_type":"code","execution_count":70,"id":"7a57e321-621d-4c67-899d-c82980b4da82","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":494,"id":"MztCvvWyDfh3","lastExecutedAt":1719734510302,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":" evaluate_prediction(test_data, \"bleu\")"},"outputs":[{"data":{"text/plain":["{'bleu': 0.2164610968786574,\n"," 'precisions': [0.5756097560975609,\n","  0.3028571428571429,\n","  0.14482758620689656,\n","  0.08695652173913043],\n"," 'brevity_penalty': 1.0,\n"," 'length_ratio': 1.1714285714285715,\n"," 'translation_length': 205,\n"," 'reference_length': 175}"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":[" evaluate_prediction(test_data, \"bleu\")"]},{"cell_type":"code","execution_count":71,"id":"165d8061-6ada-41c0-85a1-5f58f0714fa8","metadata":{"deletable":false,"editable":false,"executionCancelledAt":null,"executionTime":362,"id":"7aIBv1uODfh3","lastExecutedAt":1719734511210,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"evaluate_prediction(test_data, \"rouge\")","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[{"data":{"text/plain":["{'rouge1': 0.4677234106336273,\n"," 'rouge2': 0.25364195430758896,\n"," 'rougeL': 0.44598032027598594,\n"," 'rougeLsum': 0.4466405252241166}"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["evaluate_prediction(test_data, \"rouge\")"]},{"cell_type":"markdown","id":"1af327e4-49d9-48c9-b89a-50ae96b7c247","metadata":{"deletable":false,"editable":false,"id":"s0D-JhuDDfh3"},"source":["The scores for the partial training and the full training are so similar that it would appear that the full training does not provide any benefit in this task. However, it should be noted that the test sets in the two cases are different. More importantly, these results are indicative of the limitations of metrics such as *bleu* and *rouge* for evaluating text generation. Take, for example, the following case from the test set:\n","\n","\n","| FalseSent                 | reason1                                        | reason2                          | reason3                         |\n","|:--------------------------|:-----------------------------------------------|:---------------------------------|:--------------------------------|\n","| Beer that is drunk by humans is white | Beer is made of barley and it is a yellow drink | A beer that is drunk by humans is not white. | Beer is brown |\n","\n","The predictions obtained by the partial and full trainings and their corresponding scores are the following:\n","\n","| full training    | prediction                 | bleu     | rouge    |\n","|:-----------------|:---------------------------|---------:|---------:|\n","| no               | Beer that is drunk by humans is white  | 0.731    | 0.889    |\n","| yes              | White beer is not suitable for human consumption. | 0.000    | 0.364    |\n","\n","The text generated by the full training is a better explanation than the reason generated by the partial training, which is a mere repetition of the nonsensical statement. However, the latter obtains much better scores than the former. Metrics such as *bleu* and *rouge* do not always replace accurately the human judgement."]},{"cell_type":"code","execution_count":78,"id":"bed5340f-79b0-4437-b4f1-4c457a5c3b3b","metadata":{"executionCancelledAt":null,"executionTime":862,"lastExecutedAt":1719734769795,"lastExecutedByKernel":"93eb7e83-e761-45df-9b8a-3d4f3541291c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import nbformat\nfrom nbconvert import HTMLExporter\n\n# Load the notebook\nwith open('Pretrained_LM_Subtask_C.ipynb') as f:\n    notebook_content = nbformat.read(f, as_version=4)\n\n# Convert the notebook to HTML\nhtml_exporter = HTMLExporter()\nhtml_data, _ = html_exporter.from_notebook_node(notebook_content)\n\n# Save the HTML to a file\nwith open('notebook.html', 'w') as f:\n    f.write(html_data)\n\nprint('Notebook has been converted to HTML and saved as notebook.html')","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Notebook has been converted to HTML and saved as notebook.html\n"]}],"source":["import nbformat\n","from nbconvert import HTMLExporter\n","\n","# Load the notebook\n","with open('Pretrained_LM_Subtask_C.ipynb') as f:\n","    notebook_content = nbformat.read(f, as_version=4)\n","\n","# Convert the notebook to HTML\n","html_exporter = HTMLExporter()\n","html_data, _ = html_exporter.from_notebook_node(notebook_content)\n","\n","# Save the HTML to a file\n","with open('notebook.html', 'w') as f:\n","    f.write(html_data)\n","\n","print('Notebook has been converted to HTML and saved as notebook.html')"]}],"metadata":{"colab":{"provenance":[]},"editor":"DataLab","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
